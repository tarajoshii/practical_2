

--- Chunked Text (100 words per chunk, 20 words overlap) ---

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching searching is the most common operation performed by a database system in sql the select statement is arguably the most versatile complex baseline for efficiency is linear search start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it searching record a collection of values for attributes of a single entity instance a row of a table collection a set of records of the same

for attributes of a single entity instance a row of a table collection a set of records of the same entity type a table trivially stored in some sequential order like a list search key a value for an attribute from the entity type could be 1 attribute lists of records if each record takes up x bytes of memory then for n records we need nx bytes of memory contiguously allocated list all nx bytes are allocated as a single “chunk” of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual

single “chunk” of memory linked list each record needs x bytes additional space for 1 or 2 memory addresses individual records are linked together in a type of chain using memory addresses contiguous vs linked records contiguously allocated array front back extra storage for a memory address records linked by memory addresses linked list pros and cons arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records records had to be moved to make space linked lists are faster for inserting anywhere in the list but slower for random access

be moved to make space linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions binary search input array of values in sorted order target value output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid

left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 time complexity linear search best case target is found at the first element only 1 comparison worst case target is not in the array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is

array n comparisons therefore in the worst case linear search is on time complexity binary search best case target is found at mid 1 comparison inside the loop worst case target is not in the array log n comparisons therefore in the worst case binary search is olog n time complexity back to database searching assume data is stored on disk by column id’s value searching for a specific id fast but what if we want to search for a specific specialval only option is linear scan of that column can’t store data on disk sorted by both id and

specific specialval only option is linear scan of that column can’t store data on disk sorted by both id and specialval at the same time data would have to be duplicated → space inefficient back to database searching assume data is stored on disk by column id’s value searching for a specific id fast but what if we want to search for a specific we need an external data structure specialval to support faster searching by only option is linear scan of that column specialval than a linear scan can’t store data on disk sorted by both id and specialval

scan of that column specialval than a linear scan can’t store data on disk sorted by both id and specialval at the same time data would have to be duplicated → space inefficient what do we have in our arsenal an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval

into a sorted array slow… a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of

from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning

storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state

is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints acid properties isolation two transactions t and t are being executed at the same 2 time but cannot affect each other if both t and t are reading the data no problem 2 if t is reading the same data that t may be writing can 2 result in dirty read nonrepeatable read phantom reads isolation dirty read dirty read a transaction t is able to read a row

dirty read nonrepeatable read phantom reads isolation dirty read dirty read a transaction t is able to read a row that has been modified by another transaction t that hasn’t yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a select but get different values because another transaction t has changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and another transaction t adds or deletes rows from the set t is using figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example

running and another transaction t adds or deletes rows from the set t is using figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide example transaction transfer continued from previous slide check

update accounts set balance balance amount where accountid receiverid continued next slide example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit

amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of

over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems

easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock distributed storage 2 directions single main node distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or

1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition the cap theorem the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every

impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the

availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability

if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure ds 4300 replicating data mark

is interpreted as you must always give up something consistency availability or tolerance to failure ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the

the world you want to give them fast performance too distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes … but ok for data warehouse applications high read volumes tco gnicirp ce swa 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own

applications high read volumes tco gnicirp ce swa 78000month httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures each node has its own cpu memory and disk coordination via application layer using conventional network geographically distributed commodity hardware data replication vs partitioning replicates have partitions have a same data as main subset of the data replication common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read

to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers leaderbased replication this write could not be sent to one of the followers… only the leader leaderbased replication very common strategy relational mysql oracle sql server postgresql nosql mongodb rethinkdb realtime web apps espresso linkedin messaging brokers kafka rabbitmq how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead

to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a

are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous what happens when the leader fails challenges how do we pick a new leader node consensus strategy – perhaps based on who has the most updates use a controller node to appoint new leader and… how

strategy – perhaps based on who has the most updates use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader what happens when the leader fails more challenges if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests leader failure detection optimal timeout is tricky replication lag replication lag refers

split brain no way to resolve conflicting requests leader failure detection optimal timeout is tricky replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main

consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader but… this can create its own challenges we created followers so they

one minute of last update come from leader but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see

prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 initially the first node is a

mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 initially the first node is a leaf node and root node 42 … represent keys of some set of kv pairs leaf nodes store keys and data although data not shown inserting another key will cause the node to split insert 35 b tree m 4 leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node 2 values stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up

stay in original node remaining values moved to new node smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node b tree m 4 insert 10 27 96 the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ex 10 since 10 42 we follow the pointer to the left of 42 note none of these new values cause a node to split b tree m

pointer to the left of 42 note none of these new values cause a node to split b tree m 4 insert 30 starting at root we descend to the leftmost child we’ll call curr curr is a leaf node thus we insert 30 into curr but curr is full so we have to split create a new node to the right of curr temporarily called newnode insert newnode into the doubly linked list of leaf nodes b tree m 4 insert 30 cont’d redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange

4 insert 30 cont’d redistribute the keys copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node parent of newnode is also root so nothing else to do b tree m 4 fast forward to this state of the tree… observation the root node is full the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper insert 37 step 1 b tree m 4 insert 37 step 2 b tree m 4 when splitting an internal node we move

1 b tree m 4 insert 37 step 2 b tree m 4 when splitting an internal node we move the middle element to the parent instead of copying it in this particular tree that means we have to create a new internal node which is also now the root ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency acid transactions focuses on “data safety” considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■

“data safety” considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will conflicts are prevented by locking resources until a transaction is complete there are both read and write locks write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive optimistic concurrency transactions do not obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if

obtain locks on data when they read or write optimistic because it assumes conflicts are unlikely to occur even if there is a conflict everything will still be ok but how add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified optimistic concurrency low conflict systems backups analytical dbs etc read heavy systems the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well

arise can be handled by rolling back and rerunning a transaction that notices a conflict so optimistic concurrency works well allows for higher concurrency high conflict systems rolling back and rerunning transactions that encounter a conflict → less efficient so a locking scheme pessimistic model might be preferable nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem

as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review

system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base basically available guarantees the

may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base basically available guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state system appears to work most of the time acid alternative for distrib systems base soft state the state of the system could change over time even wo input changes could be result of eventual consistency data stores don’t have to be writeconsistent replicas don’t have to be mutually consistent acid alternative for distrib systems base eventual consistency the system

to be writeconsistent replicas don’t have to be mutually consistent acid alternative for distrib systems base eventual consistency the system will eventually become consistent all writes will eventually stop so all nodesreplicas can be updated categories of nosql dbs review first up → keyvalue databases key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its

value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results

that all nodes will eventually converge on the same value kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained

put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores redis it

used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings

commands does not handle complex data no secondary indexes only supports lookup by key redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice

not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string

commands related to setting and getting kv pairs and variations many language libraries available as well foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by

select a different database some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could

of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message

queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time linked lists crash course back front nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs

new value at front or insert new value at end list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to

standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200

cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip step 1 find the redis image open docker desktop use the built in search to find the redis image click run step 2 configure run the container give the new container a name enter 6379 in host port field click run

step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they

test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 001 port is the port mapping given when you created the container probably the default 6379

could be localhost or 001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents

be an exhaustive list of commands only some highlights check the documentation for a complete list string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 list commands

list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in

pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 redis in context redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to

a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable what is json json javascript object notation a lightweight datainterchange format it is easy for humans to read and write it is easy for machines to parse and generate json is built on two structures a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array an ordered list of values in most languages this is operationalized as an array vector list

list or associative array an ordered list of values in most languages this is operationalized as an array vector list or sequence these are two universal data structures supported by virtually all modern programming languages thus json makes a great data interchange format json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be

traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages xml extensible markup language precursor to json as data exchange format xml css → web pages that separated content and formatting structurally similar to html but tag set is extensible xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet

of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport

deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer mongodb mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 mongodb documents no predefined schema for

document 1 document 2 document 2 document 2 document 3 document 3 document 3 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased

failover load balancing built in mongodb versions mongodb atlas fully managed mongodb service in the cloud dbaas mongodb enterprise subscriptionbased selfmanaged version of mongodb mongodb community sourceavailable freetouse selfmanaged interacting with mongodb mongosh → mongodb shell cli tool for interacting with a mongodb instance mongodb compass free opensource gui to work with a mongodb database datagrip and other 3rd party tools every major language has a library to interface with mongodb pymongo python mongoose javascriptnode … mongodb community edition in docker create a container map hostcontainer port e give initial username and d password for superuser mongodb compass gui tool

docker create a container map hostcontainer port e give initial username and d password for superuser mongodb compass gui tool for interacting with mongodb instance download and install from here load mflix sample data set in compass create a new database named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database creating a database and collection to create a new db mflix users to create a new collection mongosh mongo shell find is like select collectionfind filters projections mongosh find select from users use mflix

a new collection mongosh mongo shell find is like select collectionfind filters projections mongosh find select from users use mflix dbusersfind select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year”

which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama comparison operators mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte

2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 return 0 don’t return pymongo pymongo pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid count documents in collection select count from collection demodbcollectioncountdocuments ds 4300 mongodb pymongo

cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid count documents in collection select count from collection demodbcollectioncountdocuments ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000

“python” postid collectioninsertonepostinsertedid printpostid find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache

introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it

social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry basics of graphs and graph theory what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are

group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted example labels person car relationship types drives owns liveswith marriedto properties paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated ex 1 → 2 → 6 → 5 not a path → 2 → 6 → 2 → 3 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted

of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles connected vs disconnected weighted vs unweighted directed vs undirected cyclic vs acyclic sparse vs dense trees types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be

one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding bfs vs dfs shortest path types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart centrality some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for

graph and tendency to strengthen or break apart centrality some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant

relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune neo4j query

can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday neo4j in docker compose docker compose supports multicontainer management setup is declarative using yaml dockercomposeyaml file services volumes networks etc

neo4j in docker compose docker compose supports multicontainer management setup is declarative using yaml dockercomposeyaml file services volumes networks etc command can be used to start stop or scale a number of services at one time provides a consistent method for producing an identical environment no more “well… it works on my machine interaction is mostly via command line dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 7474 7687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins env files env files stores a collection of

environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 docker compose commands to test if you have docker cli properly installed run docker version major docker commands docker compose up docker compose up d docker compose down docker compose start docker compose stop docker compose build docker compose build nocache localhost7474 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user

neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace download dataset and move to import folder clone

users were born in london match usruser birthplace “london” return usrname usrbirthplace download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport importing data basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line importing with directors this time load csv

load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind

adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p ds 4300 aws introduction mark fontenot phd northeastern university amazon web services leading cloud platform with over 200 different services available globally available via its massive networks of regions and availability zones with their massive data centers based on a payasyouuse cost model theoretically cheaper than renting rackspaceservers in a data center… theoretically history of aws

centers based on a payasyouuse cost model theoretically cheaper than renting rackspaceservers in a data center… theoretically history of aws originally launched in 2006 with only 2 services s3 ec2 by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others amazon had competitions with big prizes to spur the adoption of aws in its early days they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now aws service categories cloud models iaas more infrastructure as a service contains the basic services that

etc… 200 services now aws service categories cloud models iaas more infrastructure as a service contains the basic services that are needed to build an it infrastructure paas more platform as a service remove the need for having to manage infrastructure you can get right to deploying your app saas more software as a service provide full software apps that are run and managed by another partyvendor cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power

the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly

client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of caching services allows content to be closer to end user httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services

for cdn and other types of caching services allows content to be closer to end user httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute storage services amazon s3 simple storage service object storage in buckets highly scalable different storage classes amazon efs elastic file system simple serverless elastic “setandforget” file system amazon ebs elastic block storage highperformance block storage service amazon file cache highspeed cache for datasets stored anywhere aws backup fully managed policybased service to

highperformance block storage service amazon file cache highspeed cache for datasets stored anywhere aws backup fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services relational amazon rds amazon aurora keyvalue amazon dynamodb inmemory amazon memorydb amazon elasticache document amazon documentdb compat with mongodb graph amazon neptune analytics services amazon athena analyze petabyte scale data where it lives s3 for example amazon emr elastic mapreduce access apache spark hive presto etc aws glue discover prepare and integrate all your data amazon redshift data warehousing service amazon kinesis realtime data streaming amazon quicksight

glue discover prepare and integrate all your data amazon redshift data warehousing service amazon kinesis realtime data streaming amazon quicksight cloudnative bireporting tool ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift aws free tier allows you to gain handson experience with a subset of the services for 12

emr amazon redshift aws free tier allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well amazon ec2 750 hoursmonth specific oss and instance sizes amazon s3 5gb 20k gets 2k puts amazon rds 750 hoursmonth of db use within certain limits … so many free services ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 ec2 ec2 → elastic cloud compute scalable virtual computing in the cloud many many instance types available payasyougo

publishing ec2 ec2 ec2 → elastic cloud compute scalable virtual computing in the cloud many many instance types available payasyougo model for pricing multiple different operating systems features of ec2 elasticity easily and programmatically scale instances up or down as needed you can use one of the standard amis or provide your own ami if preconfig is needed easily integrates with many other services such as s3 rds etc ami amazon machine image ec2 lifecycle launch when starting an instance for the first time with a chosen configuration startstop temporarily suspend usage without deleting the instance terminate permanently delete the

for the first time with a chosen configuration startstop temporarily suspend usage without deleting the instance terminate permanently delete the instance reboot restart an instance without sling the data on the root volume where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even common ec2 use cases web hosting run a websiteweb server and associated apps data processing it’s a vm… you can do anything to data possible with a programming

websiteweb server and associated apps data processing it’s a vm… you can do anything to data possible with a programming language machine learning train models using gpu instances disaster recovery backup critical workloads or infrastructure in the cloud let’s spin up an ec2 instance let’s spin up an ec2 instance let’s spin up an ec2 instance ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade miniconda on ec2 make sure you’re logged in to your ec2

the packages installed sudo apt update sudo apt upgrade miniconda on ec2 make sure you’re logged in to your ec2 instance let’s install miniconda curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh bash miniconda3latestlinuxx8664sh installing using streamlit log out of your ec2 instance and log back in make sure pip is now available pip version install streamlit and sklearn pip install streamlit scikitlearn make a directory for a small web app mkdir web cd web basic streamlit app import streamlit as st def main nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ctrlx to

testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ctrlx to save and exit data set 02 data set 03 streamlit run testpy stwriten stwrite goodbye if name main main opening up the streamlit port in a browser aws lambda lambdas lambdas provide serverless computing automatically run code in response to events relieves you from having to manager servers only worry about the code you only pay for execution time not for idle compute time different from ec2 lambda features eventdriven execution can be triggered by many different events in aws

for idle compute time different from ec2 lambda features eventdriven execution can be triggered by many different events in aws supports a large number of runtimes… python java nodejs etc highly integrated with other aws services extremely scalable and can rapidly adjust to demands how it works addupload your code through aws mgmt console configure event sources watch your lambda run when one of the event sources fires an event let’s make one making a lambda creating a function sample code edit the code deploy the code test it 1325 525 pm btrees btrees the idea we saw earlier of

code edit the code deploy the code test it 1325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory

node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the

trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants every path from

the maximum number may be greater or typically less than m the data structure satisfies several invariants every path from the root to a leaf has the same length if a node has n children it contains n−1 keys every node except the root is at least half full the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant the root has at least two children if it is not a leaf for example the following is an order5

the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a

course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html

or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 1325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two

the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf

one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if

parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11

reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 1325 525 pm 126 btrees — cs3 data structures algorithms 6 btrees 61 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file

standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record update and search operations affect only those disk blocks on the path from the root to the leaf node

record update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search

a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally

is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will

from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 20 33 45 48 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the

a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 1325 525 pm 126 btrees — cs3 data structures algorithms otherwise follow the proper branch and repeat the process for example consider a search for the record with key

algorithms otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room

is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children

attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency

never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how

based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to

block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could

the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a

chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer

different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than m records the requirement is simply that the leaf nodes

might have enough room to store more or less than m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean

leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 1325 525 pm 126 btrees

with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 1325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 28 example 23 tree visualization insert

works this would be the “2−3 tree” or a b tree of order 3 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 10 example 23 tree visualization search 65 52 71 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 1325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 65

tree 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 1325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 65 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary

remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 10 example b tree visualization search in a tree of degree 4 40 98 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always

search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 1325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b

1325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split

leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k

four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 1325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values

basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 60 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need

b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 23 example b tree visualization delete from a tree of degree 4 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 1325 525 pm 126 btrees — cs3 data structures algorithms 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called

of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this

records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute

room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key

delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is

satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and

the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five

always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 1325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the

that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer

of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider

factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have

it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is

more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 4 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as

means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times

using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward

of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 1325 525 pm 126 btrees — cs3 data

required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 1325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we

belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node an example 12 6 9 19 5 8 11 15 20 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the

a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies inorder the ordering is the left subtree the current node the right subtree preorder the ordering is the current node the left subtree the right subtree postorder the ordering is the left subtree the right subtree the current node inorder traversal pseudocode this recursive algorithm takes

postorder the ordering is the left subtree the right subtree the current node inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx if x nil then return inorderwalkleftx print keyx inorderwalkrightx we can write a similar pseudocode for preorder and postorder 1 3 3 2 3 1 2 inorder preorder postorder 12 6 9 19 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about

6 9 19 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal inorder traversal gives 2 3 5 6 7 8 9 11 12 15 20 preorder traversal gives 7 4 3 6 5 12 9 8 11 19 20 postorder traversal gives 3 5 6 4 8 11 9 15 20 12 7 so inorder travel on a bst finds the keys in nondecreasing order operations on bst searching for a key we assume that a key and the subtree in which the key is searched

on bst searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th

that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree algorithm here k is the key that is searched for and x is the start node bstsearchx k y x ← while y nil do ̸ if keyy k then return y

is the start node bstsearchx k y x ← while y nil do ̸ if keyy k then return y else if keyy k then y righty ← else y lefty ← return “not found” an example search for 8 11 6 9 13 nil what is the running time of search the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx if x

maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx if x nil then return “empty tree” y x ← while lefty nil do y lefty ̸ ← return keyy bstmaximumx if x nil then return “empty tree” y x ← while righty nil do y righty ̸ ← return keyy insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty bstinsertx z k if x nil then return “error”

such that replacing it by z does not break the bstproperty bstinsertx z k if x nil then return “error” y x ← while true do if keyy k then z lefty ← else z righty ← if z nil break if keyy k then lefty z ← else rightpy z ← the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a

the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward an example 12 6 9 19 5 8 11 15 20 algorithm bstsuccessorx if rightx nil then ̸ y rightx ← while lefty nil do y lefty ̸ ← return y else y x ← while rightpx

̸ y rightx ← while lefty nil do y lefty ̸ ← return y else y x ← while rightpx x do y px ← if px nil then return px ̸ else return “no successor” the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm deletion suppose we want to delete a node z if z has no children then we will just replace z by nil if z has

delete a node z if z has no children then we will just replace z by nil if z has only one child then we will promote the unique child to z’s place if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 8 11 5 11 6 9 13 1 6 9 13 7 10 3 10 4 2 4 8 11 5 11 6

11 6 9 13 1 6 9 13 7 10 3 10 4 2 4 8 11 5 11 6 9 13 3 6 9 13 7 10 2 4 7 10 4 9 11 5 11 6 9 13 1 6 10 13 7 10 3 4 2 4 algorithm this algorithm deletes z from bst t bstdeletet z if leftz nil or rightz nil then y z ← else y bstsuccessorz ← ✄ y is the node that’s actually removed ✄ here y does not have two children if lefty nil ̸ then x lefty ← else x

actually removed ✄ here y does not have two children if lefty nil ̸ then x lefty ← else x righty ← ✄ x is the node that’s moving to y’s position if x nil then px py ̸ ← ✄ px is reset if x isn’t nil ✄ resetting is unnecessary if x is nil algorithm cont’d if py nil then roott x ← ✄ if y is the root then x becomes the root ✄ otherwise do the following else if y leftpy then leftpy x ← ✄ if y is the left child of its parent then

following else if y leftpy then leftpy x ← ✄ if y is the left child of its parent then ✄ set the parent’s left child to x else rightpy x ← ✄ if y is the right child of its parent then ✄ set the parent’s right child to x if y z then ̸ keyz keyy ← move other data from y to z return y summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time randomly built

height h search minimum maximum successor predecessor insert and delete can be made to run in oh time randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n

actually values of the keys do not matter what matters is the position of the inserted key in the n keys the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of

the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − −

⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 − ey ey n i ≤ n i1 analysis n3 we claim that for all n 1 ey n 3 ≥ ≤ we prove this by induction on n ’ base case ey 2 1 induction step we have n 1 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 4 i0 ’ ’ 1 n 3

n i1 using the fact that n 1 i 3 n 3 − 4 i0 ’ ’ 1 n 3 ey n ≤ n · 4 · 4 ’ n 3 ey n ≤ 4 · 3 ’ jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n

and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n n 3 ex n ≤ 4 3 ’ the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 1325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46

46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys

keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes —

is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in

edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just

you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you

every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate

recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as

you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously

of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced

if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of

search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 1325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies

respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might

first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height

a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the

possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be

binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of

trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree

wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even

n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses

when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that

worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 1325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which

tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found

giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an

definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property

nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider

maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be

is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above

of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever

the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we

improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any

triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 1325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 2 3 all keys in t are

and t maintain the appropriate positions relative to the keys a and b 2 3 all keys in t are smaller than a all keys in t are larger than a and smaller than b all keys in t are larger than b performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would

θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t bs left child would now be the root of t instead of a a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still

an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 1325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key

an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 1325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we

lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do

left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and

more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were

path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the

the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would

two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t the empty left subtree of the node containing 35 is t the empty right subtree of the node containing 35 is t the empty right subtree of the node

is t the empty right subtree of the node containing 35 is t the empty right subtree of the node containing 40 is t httpsicsucieduthorntonics46notesavltrees 57 1325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the

of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start

by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog

key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single

time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if

time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share

turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an

of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for

a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh

to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out

more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 log mh ≥ h finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between

number of nodes in the tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 1325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower

of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77 ds4300 midterm prep rag guide what is the difference between a list where memory is contiguously allocated and a list where linked structures are used contiguously allocated lists like arrays store elements in backtoback memory locations pros fast random access

where linked structures are used contiguously allocated lists like arrays store elements in backtoback memory locations pros fast random access via indexing cons insertionsdeletions especially in the middle require shifting elements linked lists store elements as nodes connected by pointers pros insertionsdeletions are efficient if the node is known cons slower access no indexing due to traversal when are linked lists faster than contiguouslyallocated lists when frequent insertions or deletions happen especially at the beginning or middle of the list when list size is unpredictable so avoiding reallocation overhead is valuable random access isnt needed and traversalbased access is sufficient

when list size is unpredictable so avoiding reallocation overhead is valuable random access isnt needed and traversalbased access is sufficient add 23 to the avl tree below what imbalance case is created 35 inserting 23 → goes to 30 → 25 → 20 → inserted as right child of 20 this causes a leftright lr imbalance at node 25 fix rotate left at 20 then right at 25 why is a b tree better than an avl tree for large datasets b trees designed for diskbased access all data in leaf nodes — fast range queries higher fanout shallower trees

datasets b trees designed for diskbased access all data in leaf nodes — fast range queries higher fanout shallower trees → fewer disk ios avl trees balanced in memory not optimized for disk access or range scanning what is diskbased indexing and why is it important for databases storing indexes on disk instead of ram used for very large datasets indexing structures like btrees minimize disk reads enables fast lookup insertion and deletion on persistent storage in a relational db what is a transaction a transaction is a sequence of operations performed as a single unit either all operations succeed

what is a transaction a transaction is a sequence of operations performed as a single unit either all operations succeed commit or none are applied rollback four components of acid transactions a atomicity allornothing execution c consistency maintains data integrity rules i isolation transactions don’t interfere with each other d durability committed changes persist even during crashes why does cap not apply to singlenode mongodb cap theorem applies to distributed systems in a singlenode mongodb no partitioning possible so the tradeoff between consistency availability and partition tolerance doesn’t make sense horizontal vs vertical scaling horizontal scaling add more machines scale

tradeoff between consistency availability and partition tolerance doesn’t make sense horizontal vs vertical scaling horizontal scaling add more machines scale out pros more scalable faulttolerant vertical scaling upgrade existing machine’s hardware pros simpler but limited by hardware how can a keyvalue store act as a feature store use entity ids as keys and feature vectors as values fast retrieval of precomputed features during model inference when was redis released redis was released in 2009 redis inc vs incr incr increments the value at a key by 1 inc not a valid redis command bson vs json in mongodb bson binary

the value at a key by 1 inc not a valid redis command bson vs json in mongodb bson binary format supports more types eg date binary more efficient for storage and transmission json textbased easier to read but limited in type support mongodb query suspense movies between 2010–2015 dbmoviesfind year gte 2010 lte 2015 genre suspense title 1 id 0 what does nin mean in mongodb nin not in — matches values not in a given array example rating nin 3 4 matches documents where rating is not 3 or 4

nin 3 4 matches documents where rating is not 3 or 4