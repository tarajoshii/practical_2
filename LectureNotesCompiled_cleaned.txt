

=== Chunk Size: 200, Overlap: 0 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked

together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid

right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure

specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational

model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t

are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where

accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13

scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that

it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute

latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability

highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now

trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new

leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that

all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys

of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split

○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses

on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ●

high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the

latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are

designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be

available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would

not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the

value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list

except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200

“sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy

in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam

nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is

easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements

from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is

needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database

named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all

movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in

2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is

a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding

the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon

neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ●

interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j

12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20

gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas

more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of

caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting

tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can

use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands

with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle

compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location

takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that

are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer

httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if

the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern

file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root

has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it

is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full

then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the

list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big

problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than

m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically

store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records

in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found

if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if

you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must

do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the

tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting

a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node

split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records

two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very

low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99

chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering

is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys

in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5

else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9

if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for

which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4

✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16 else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete

can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y

2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that

for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the

trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of

a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty

binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a

burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree

with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level

of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even

worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come

up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont

exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than

logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an

avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called

rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are

larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these

rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer

lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the

right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing

40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on

the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes

if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with

one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure

out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt

change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

=== Chunk Size: 200, Overlap: 50 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked

of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right

lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored

○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we

specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational

binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are

work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8

modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback

accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13

to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new

no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain

tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give

latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability

for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ●

the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible

a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout

leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that

important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them

into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert

of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split ○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange

○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses

particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has

conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ● high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view

meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can

latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are

11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session

store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure

dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅

not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the

set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in

weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating

lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the

“sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy

there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client

next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result

prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○

easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements

supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for

a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb

indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like

named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all

5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo

collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database

the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges

a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding

directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance

19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns

dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ● interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment

interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j

london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j 12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load

17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20 gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block

zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them

more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of

oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ●

○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon

s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop

use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands

learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14

import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code

works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the

takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that

satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then

of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and

in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be

the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern

e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this

disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node

in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key

is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full

is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the

the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important

of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a

problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than

a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key

detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example

insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127

in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found

int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at

42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains

the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have

do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the

had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become

note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization

nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search

split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records

a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has

are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several

require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the

chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering

the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5

2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst

the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5 else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can

else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9

that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9 if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else

following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above

promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4 ✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py

✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16 else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete

the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x

view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n

− − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3

for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the

tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order

will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows

from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you

binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a

to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are

in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height

things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore

of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even

trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if

to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide

good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants

exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than

basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty

theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property

squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is

rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are

are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead

kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem

search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of

lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the

inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a

40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing 40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still

contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions

the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes

what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a

a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution

bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of

out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt

n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

=== Chunk Size: 200, Overlap: 100 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked

a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked

together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid

lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored

right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure

on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow…

specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational

2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of

model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t

work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8

are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where

isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if

accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13

end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently

scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that

no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain

it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute

operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance

latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability

high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to

highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now

the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible

trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new

because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication

leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that

we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far

all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys

into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert

of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split

10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split ○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root

○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses

to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes

on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ●

conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ● high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view

high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the

of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency

latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are

○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda

designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be

store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure

available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would

originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation

not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the

data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price

value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list

weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating

except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200

a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start

“sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy

datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the

in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam

next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result

nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is

true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson

easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements

→ binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of

from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is

a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb

needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database

● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released

named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all

in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid

movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in

collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database

2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is

data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 →

a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding

6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality

the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon

19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns

neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ●

and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ● interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose

interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j

down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j 12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line

12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20

17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20 gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block

gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas

store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the

more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of

shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup

caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting

○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon

tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can

s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage

use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands

ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app

with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle

import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code

compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location

24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which

takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that

each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors

are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer

of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and

httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if

child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the

the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern

tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less

file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root

disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node

has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it

usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root

is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full

node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually

then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the

the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important

list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big

since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to

problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than

guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation

m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically

detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example

store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records

of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search

in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found

must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1

if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if

42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains

you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must

r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value

do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the

in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but

tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting

note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization

a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node

insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or

split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records

more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there

two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very

are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several

low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99

nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers

chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering

to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4

is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys

2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst

in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5

property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5 else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return

else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9

keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9 if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by

if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for

following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above

which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4

two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4 ✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16

✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16 else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete

else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will

can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y

view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n

2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that

’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and

for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the

examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape

trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of

will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows

a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty

this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would

binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a

be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree

burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree

in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height

with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level

h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see

of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even

that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take

worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come

to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide

up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont

on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good

exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than

balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees

logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an

theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property

avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called

avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary

rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are

subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second

larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these

kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem

rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer

is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to

lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the

store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from

right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing

40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing 40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still

40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on

empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog

the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes

n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of

if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with

a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution

one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure

technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325

out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt

524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

=== Chunk Size: 500, Overlap: 0 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored

on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t

are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently

no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability

highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication

we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split

○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ● high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view

of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be

available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price

weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy

in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson

→ binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database

named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database

data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon

neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ● interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j 12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line

17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20 gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of

caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage

ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location

takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and

child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root

has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually

the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than

m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search

must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must

do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization

insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very

low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4

2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5 else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9

if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4 ✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16

else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that

for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows

this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree

with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take

to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than

logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary

subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer

lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing 40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still

empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with

one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

=== Chunk Size: 500, Overlap: 50 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored

○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are

work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13

to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give

latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible

a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them

into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split ○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses

particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ● high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can

latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure

dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in

weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy

there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○

easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb

indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo

collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding

directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ● interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment

interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j 12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20 gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block

zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ●

○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands

learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the

takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and

in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this

disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full

is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a

problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example

insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at

42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the

had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search

split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several

require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5

2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5 else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9

that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9 if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4 ✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py

✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16 else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n

− − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order

will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a

to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore

of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide

good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty

theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are

are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of

lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing 40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still

contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a

a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt

n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

=== Chunk Size: 500, Overlap: 100 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored

right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of

model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if

accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain

it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to

highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication

leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert

of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split ○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes

on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ● high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency

latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure

available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price

value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start

“sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result

nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of

from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released

named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database

2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality

the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ● interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose

interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j 12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20 gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block

gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup

caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage

use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code

compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors

are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the

the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node

has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually

then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to

problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example

store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1

if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value

do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization

a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there

two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers

chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst

in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5 else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9 if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by

if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4 ✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16

✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16 else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n

2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape

trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would

binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height

with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take

worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good

exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property

avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second

larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to

lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing 40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still

40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of

if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325

out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

=== Chunk Size: 1000, Overlap: 0 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t

are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability

highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split

○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ● high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be

available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy

in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database

named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon

neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ● interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j 12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20 gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of

caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location

takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root

has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than

m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must

do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very

low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5 else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9

if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4 ✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16 else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that

for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree

with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than

logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer

lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing 40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with

one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

=== Chunk Size: 1000, Overlap: 50 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t

executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility

high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert

of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split ○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ● high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are

designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200

and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for

a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 →

not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ● interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j 12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20 gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability

gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands

learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a

child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually

that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127

in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting

so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all

to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5 else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9 if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above

promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4 ✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16 else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is

trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level

− 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty

theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to

each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing 40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a

one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77

=== Chunk Size: 1000, Overlap: 100 ===

ds 4300 large scale information storage and retrieval foundations mark fontenot phd northeastern university searching ● searching is the most common operation performed by a database system ● in sql the select statement is arguably the most versatile complex ● baseline for efficiency is linear search ○ start at the beginning of a list and proceed element by element until ■ you find what you’re looking for ■ you get to the last element and haven’t found it 2 searching ● record a collection of values for attributes of a single entity instance a row of a table ● collection a set of records of the same entity type a table ○ trivially stored in some sequential order like a list ● search key a value for an attribute from the entity type ○ could be 1 attribute 3 lists of records ● if each record takes up x bytes of memory then for n records we need nx bytes of memory ● contiguously allocated list ○ all nx bytes are allocated as a single “chunk” of memory ● linked list ○ each record needs x bytes additional space for 1 or 2 memory addresses ○ individual records are linked together in a type of chain using memory addresses 4 contiguous vs linked 6 records contiguously allocated array front back extra storage for a memory address 6 records linked by memory addresses linked list 5 pros and cons ● arrays are faster for random access but slow for inserting anywhere but the end records insert after 2nd record records 5 records had to be moved to make space ● linked lists are faster for inserting anywhere in the list but slower for random access insert after 2nd record 6 observations arrays fast for random access slow for random insertions linked lists slow for random access fast for random insertions 7 binary search ● input array of values in sorted order target value ● output the location index of where target is located or some value indicating target was not found def binarysearcharr target left right 0 lenarr 1 left right while left right a c g m p r z target a mid left right 2 if arrmid target mid return mid since target arrmid we reset right to mid 1 left right elif arrmid target left mid 1 a c g m p r z target a else mid right mid 1 return 1 8 time complexity ● linear search ○ best case target is found at the first element only 1 comparison ○ worst case target is not in the array n comparisons ○ therefore in the worst case linear search is on time complexity ● binary search ○ best case target is found at mid 1 comparison inside the loop ○ worst case target is not in the array log n comparisons 2 ○ therefore in the worst case binary search is olog n time 2 complexity 9 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific specialval ○ only option is linear scan of that column ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 10 back to database searching ● assume data is stored on disk by column id’s value ● searching for a specific id fast ● but what if we want to search for a specific we need an external data structure specialval to support faster searching by ○ only option is linear scan of that column specialval than a linear scan ● can’t store data on disk sorted by both id and specialval at the same time ○ data would have to be duplicated → space inefficient 11 what do we have in our arsenal 1 an array of tuples specialval rownumber sorted by specialval a we could use binary search to quickly locate a particular specialval and find its corresponding row in the table b but every insert into the table would be like inserting into a sorted array slow… 2 a linked list of tuples specialval rownumber sorted by specialval a searching for a specialval would be slow linear scan required b but inserting into the table would theoretically be quick to also add to the list 12 something with fast insert and fast search binary search tree a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent image from httpscoursesgraingerillinoiseducs225sp2019notesbst 13 to the board 14 ds 4300 moving beyond the relational model mark fontenot phd northeastern university benefits of the relational model mostly standard data model and query language acid compliance more on this in a second atomicity consistency isolation durability works well will highly structured data can handle large amounts of data well understood lots of tooling lots of experience 2 relational database performance many ways that a rdbms increases efficiency indexing the topic we focused on directly controlling storage column oriented storage vs row oriented storage query optimization cachingprefetching materialized views precompiled stored procedures data replication and partitioning 3 transaction processing transaction a sequence of one or more of the crud operations performed as a single logical unit of work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t

work either the entire sequence succeeds commit or the entire sequence fails rollback or abort help ensure data integrity error recovery concurrency control reliable data storage simplified error handling 4 acid properties atomicity transaction is treated as an atomic unit it is fully executed or no parts of it are executed consistency a transaction takes a database from one consistent state to another consistent state consistent state all data meets integrity constraints 5 acid properties isolation two transactions t and t are being executed at the same 1 2 time but cannot affect each other if both t and t are reading the data no problem 1 2 if t is reading the same data that t may be writing can 1 2 result in dirty read nonrepeatable read phantom reads 6 isolation dirty read dirty read a transaction t is able 1 to read a row that has been modified by another transaction t that hasn’t 2 yet executed a commit figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 7 isolation nonrepeatable read nonrepeatable read two queries in a single transaction t execute a 1 select but get different values because another transaction t has 2 changed data and committed figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 8 isolation phantom reads phantom reads when a transaction t is running and 1 another transaction t adds or 2 deletes rows from the set t is using 1 figure from httpswwwmybluelinuxcomrelationaldatabasesexplained 9 example transaction transfer delimiter create procedure transfer in senderid int in receiverid int in amount decimal102 begin declare rollbackmessage varchar255 default transaction rolled back insufficient funds declare commitmessage varchar255 default transaction committed successfully start the transaction start transaction attempt to debit money from account 1 update accounts set balance balance amount where accountid senderid attempt to credit money to account 2 update accounts set balance balance amount where accountid receiverid continued next slide 10 example transaction transfer continued from previous slide check if there are sufficient funds in account 1 simulate a condition where there are insufficient funds if select balance from accounts where accountid senderid 0 then roll back the transaction if there are insufficient funds rollback signal sqlstate 45000 45000 is unhandled userdefined error set messagetext rollbackmessage else log the transactions if there are sufficient funds insert into transactions accountid amount transactiontype values senderid amount withdrawal insert into transactions accountid amount transactiontype values receiverid amount deposit commit the transaction commit select commitmessage as result end if end delimiter 11 acid properties durability once a transaction is completed and committed successfully its changes are permanent even in the event of a system failure committed transactions are preserved for more info on transactions see kleppmann book chapter 7 12 but … relational databases may not be the solution to all problems… sometimes schemas evolve over time not all apps may need the full strength of acid compliance joins can be expensive a lot of data is semistructured or unstructured json xml etc horizontal scaling presents challenges some apps need something more performant real time low latency systems 13 scalability up or out conventional wisdom scale vertically up with bigger more powerful systems until the demands of highavailability make it necessary to scale out with some type of distributed computing model but why scaling up is easier no need to really modify your architecture but there are practical and financial limits however there are modern systems that make horizontal scaling less problematic 14 so what distributed data when scaling out a distributed system is “a collection of independent computers that appear to its users as one computer” andrew tennenbaum characteristics of distributed systems computers operate concurrently computers fail independently no shared global clock 15 distributed storage 2 directions single main node 16 distributed data stores data is stored on 1 node typically replicated ie each block of data is available on n nodes distributed databases can be relational or nonrelational mysql and postgresql support replication and sharding cockroachdb new player on the scene many nosql systems support one or both models but remember network partitioning is inevitable network failures system failures overall system needs to be partition tolerant system can keep running even w network partition 17 the cap theorem 18 the cap theorem the cap theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees consistency every read receives the most recent write or error thrown availability every request receives a nonerror response but no guarantee that the response contains the most recent write partition tolerance the system can continue to operate despite arbitrary network issues 19 cap theorem database view consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 20 cap theorem database view consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network issues consistency partition tolerance if system responds with data from a distributed store it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance

latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 21 cap in reality what it is really saying if you cannot limit the number of faults requests can be directed to any server and you insist on serving every request then you cannot possibly be consistent but it is interpreted as you must always give up something consistency availability or tolerance to failure 22 23 ds 4300 replicating data mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributing data benefits scalability high throughput data volume or readwrite load grows beyond the capacity of a single machine fault tolerance high availability your application needs to continue working even if one or more machines goes down latency when you have users in different parts of the world you want to give them fast performance too 2 distributed data challenges consistency updates must be propagated across the network application complexity responsibility for reading and writing data in a distributed environment often falls to the application 3 vertical scaling shared memory architectures geographically centralized server some fault tolerance via hotswappable components 4 vertical scaling shared disk architectures machines are connected via a fast network contention and the overhead of locking limit scalability highwrite volumes … but ok for data warehouse applications high read volumes 5 4202 tco gnicirp 2ce swa 78000month 6 httpsawsamazoncomec2pricingondemand horizontal scaling shared nothing architectures ● each node has its own cpu memory and disk ● coordination via application layer using conventional network ● geographically distributed ● commodity hardware 7 data replication vs partitioning replicates have partitions have a same data as main subset of the data 8 replication 9 common strategies for replication single leader model multiple leader model leaderless model distributed databases usually adopt one of these strategies 10 leaderbased replication all writes from clients go to the leader leader sends replication info to the followers followers process the instructions from the leader clients can read from either the leader or followers 11 leaderbased replication this write could not be sent to one of the followers… only the leader 12 leaderbased replication very common strategy relational ● mysql ● oracle ● sql server ● postgresql nosql ● mongodb ● rethinkdb realtime web apps ● espresso linkedin messaging brokers kafka rabbitmq 13 how is replication info transmitted to followers replication method description statementbased send insert update deletes to replica simple but errorprone due to nondeterministic functions like now trigger sideeffects and difficulty in handling concurrent transactions writeahead log wal a bytelevel specific log of every change to the database leader and all followers must implement the same storage engine and makes upgrades difficult logical rowbased log for relational dbs inserted rows modified rows before and after deleted rows a transaction log will identify all the rows that changed in each transaction and how they changed logical logs are decoupled from the storage engine and easier to parse triggerbased changes are logged to a separate table whenever a trigger fires in response to an insert update or delete flexible because you can have application specific replication but also more error prone 14 synchronous vs asynchronous replication synchronous leader waits for a response from the follower asynchronous leader doesn’t wait for confirmation synchronous asynchronous 15 what happens when the leader fails challenges how do we pick a new leader node ● consensus strategy – perhaps based on who has the most updates ● use a controller node to appoint new leader and… how do we configure clients to start writing to the new leader 16 what happens when the leader fails more challenges ● if asynchronous replication is used new leader may not have all the writes how do we recover the lost writes or do we simply discard ● after if the old leader recovers how do we avoid having multiple leaders receiving conflicting data split brain no way to resolve conflicting requests ● leader failure detection optimal timeout is tricky 17 replication lag replication lag refers to the time it takes for writes on the leader to be reflected on all of the followers ● synchronous replication replication lag causes writes to be slower and the system to be more brittle as num followers increases ● asynchronous replication we maintain availability but at the cost of delayed or eventual consistency this delay is called the inconsistency window 18 readafterwrite consistency scenario you’re adding a comment to a reddit post… after you click submit and are back at the main post your comment should show up for you less important for other users to see your comment as immediately 19 implementing readafterwrite consistency method 1 modifiable data from the client’s perspective is always read from the leader 20 implementing readafterwrite consistency method 2 dynamically switch to reading from leader for “recently updated” data for example have a policy that all requests within one minute of last update come from leader 21 but… this can create its own challenges we created followers so they would be proximal to users but… now we have to route requests to distant leaders when reading modifiable data 22 monotonic read consistency monotonic read anomalies occur when a user reads values out of order from multiple followers monotonic read consistency ensures that when a user makes multiple reads they will not read older data after previously reading newer data 23 consistent prefix reads reading data out of order can occur if different partitions how far into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys

into the future can you see ms b replicate data at different a rates there is no global write consistency consistent prefix read about 10 seconds usually mr a b guarantee ensures that if a sequence of writes happens in a certain order anyone reading those writes will see them appear in the same order 24 25 ds 4300 large scale information storage and retrieval b tree walkthrough mark fontenot phd northeastern university insert 42 21 63 89 b tree m 4 ● initially the first node is a leaf node and root node ● 21 42 … represent keys of some set of kv pairs ● leaf nodes store keys and data although data not shown ● inserting another key will cause the node to split 2 insert 35 b tree m 4 ● leaf node needs to split to accommodate 35 new leaf node allocated to the right of existing node ● 52 values stay in original node remaining values moved to new node ● smallest value from new leaf node 42 is copied up to the parent which needs to be created in this case it will be an internal node 3 b tree m 4 insert 10 27 96 ● the insert process starts at the root node the keys of the root node are searched to find out which child node we need to descend to ○ ex 10 since 10 42 we follow the pointer to the left of 42 ● note none of these new values cause a node to split 4 b tree m 4 insert 30 ● starting at root we descend to the leftmost child we’ll call curr ○ curr is a leaf node thus we insert 30 into curr ○ but curr is full so we have to split ○ create a new node to the right of curr temporarily called newnode ○ insert newnode into the doubly linked list of leaf nodes 5 b tree m 4 insert 30 cont’d ● redistribute the keys ● copy the smallest key 27 in this case from newnode to parent rearrange keys and pointers in parent node ● parent of newnode is also root so nothing else to do 6 b tree m 4 fast forward to this state of the tree… ● observation the root node is full ○ the next insertion that splits a leaf will cause the root to split and thus the tree will get 1 level deeper 7 insert 37 step 1 b tree m 4 8 insert 37 step 2 b tree m 4 ● when splitting an internal node we move the middle element to the parent instead of copying it ● in this particular tree that means we have to create a new internal node which is also now the root 9 ds 4300 nosql kv dbs mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks distributed dbs and acid pessimistic concurrency ● acid transactions ○ focuses on “data safety” ○ considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions ■ iow it assumes that if something can go wrong it will ○ conflicts are prevented by locking resources until a transaction is complete there are both read and write locks ○ write lock analogy → borrowing a book from a library… if you have it no one else can see httpswwwfreecodecamporgnewshowdatabasesguaranteeisolation for more for a deeper dive 2 optimistic concurrency ● transactions do not obtain locks on data when they read or write ● optimistic because it assumes conflicts are unlikely to occur ○ even if there is a conflict everything will still be ok ● but how ○ add last update timestamp and version number columns to every table… read them when changing then check at the end of transaction to see if any other transaction has caused them to be modified 3 optimistic concurrency ● low conflict systems backups analytical dbs etc ○ read heavy systems ○ the conflicts that arise can be handled by rolling back and rerunning a transaction that notices a conflict ○ so optimistic concurrency works well allows for higher concurrency ● high conflict systems ○ rolling back and rerunning transactions that encounter a conflict → less efficient ○ so a locking scheme pessimistic model might be preferable 4 nosql “nosql” first used in 1998 by carlo strozzi to describe his relational database system that did not use sql more common modern meaning is “not only sql” but sometimes thought of as nonrelational dbs idea originally developed in part as a response to processing unstructured webbased data httpswwwdataversitynetabriefhistoryofnonrelationaldatabases 5 cap theorem review you can have 2 but not 3 of the following consistency every user of the db has an identical view of the data at any given instant availability in the event of a failure the database system remains operational partition tolerance the database can maintain operations in the event of the network’s failing between two segments of the distributed system note the definition of consistency in cap is different from that of acid reference httpsalperenbayramoglucompostsunderstandingcaptheorem 6 cap theorem review consistency availability system always responds with the latest data and every request gets a response but may not be able to deal with network partitions consistency partition tolerance if system responds with data from the distrib system it is always the latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency

latest else data request is dropped availability partition tolerance system always sends are responds based on distributed store but may not be the absolute latest data reference httpsalperenbayramoglucompostsunderstandingcaptheorem 7 acid alternative for distrib systems base ● basically available ○ guarantees the availability of the data per cap but response can be “failure”“unreliable” because the data is in an inconsistent or changing state ○ system appears to work most of the time 8 acid alternative for distrib systems base ● soft state the state of the system could change over time even wo input changes could be result of eventual consistency ○ data stores don’t have to be writeconsistent ○ replicas don’t have to be mutually consistent 9 acid alternative for distrib systems base ● eventual consistency the system will eventually become consistent ○ all writes will eventually stop so all nodesreplicas can be updated 10 categories of nosql dbs review 11 first up → keyvalue databases 12 key value stores key value keyvalue stores are designed around simplicity the data model is extremely simple comparatively tables in a rdbms are very complex lends itself to simple crud ops and api creation 13 key value stores key value keyvalue stores are designed around speed usually deployed as inmemory db retrieving a value given its key is typically a o1 op bc hash tables or similar data structs used under the hood no concept of complex queries or joins… they slow things down 14 key value stores key value keyvalue stores are designed around scalability horizontal scaling is simple add more nodes typically concerned with eventual consistency meaning in a distributed environment the only guarantee is that all nodes will eventually converge on the same value 15 kv ds use cases edaexperimentation results store store intermediate results from data preprocessing and eda store experiment or testing ab results wo prod db feature store store frequently accessed feature → lowlatency retrieval for model training and prediction model monitoring store key metrics about performance of model for example in realtime inferencing 16 kv swe use cases storing session information everything about the current session can be stored via a single put or post and retrieved with a single get … very fast user profiles preferences user info could be obtained with a single get operation… language tz product or ui preferences shopping cart data cart data is tied to the user needs to be available across browsers machines sessions caching layer in front of a diskbased database 17 redis db redis remote directory server open source inmemory database sometimes called a data structure store primarily a kv store but can be used with other models graph spatial full text search vector time series from dbenginescom ranking of kv stores 18 redis it is considered an inmemory database system but… supports durability of data by a essentially saving snapshots to disk at specific intervals or b appendonly file which is a journal of changes that can be used for rollforward if there is a failure originally developed in 2009 in c can be very fast … 100000 set ops second rich collection of commands does not handle complex data no secondary indexes only supports lookup by key 19 redis data types keys usually strings but can be any binary sequence values strings lists linked lists sets unique unsorted string elements sorted sets hashes string → string geospatial data 20 setting up redis in docker in docker desktop search for redis pullrun the latest image see above optional settings add 6379 to ports to expose that port so we can connect to it normally you would not expose the redis port for security reasons if you did this in a prod environment major security hole notice we didn’t set a password… 21 connecting from datagrip file new data source redis give the data source a name make sure the port is 6379 test the connection ✅ 22 redis database and interaction redis provides 16 databases by default they are numbered 0 to 15 there is no other name associated direct interaction with redis is through a set of commands related to setting and getting kv pairs and variations many language libraries available as well 23 foundation data type string sequence of bytes text serialized objects bin arrays simplest data type maps a string to another string use cases caching frequently accessed htmlcssjs fragments config settings user settings info token management counting web pageapp screen views or rate limiting 24 some initial basic commands set pathtoresource 0 set user1 “john doe” get pathtoresource exists user1 del user1 keys user select 5 select a different database 25 some basic commands set somevalue 0 incr somevalue increment by 1 incrby somevalue 10 increment by 10 decr somevalue decrement by 1 decrby somevalue 5 decrement by 5 incr parses the value as int and increments or adds to value setnx key value only sets value to key if key does not already exist 26 hash type value of kv entry is a collection of fieldvalue pairs use cases can be used to represent basic objectsstructures number of fieldvalue pairs per hash is 2321 practical limit available system resources eg memory session information management userevent tracking could include ttl active session tracking all sessions under one hash key 27 hash commands hset bike1 model demios brand ergonom price 1971 hget bike1 model hget bike1 price hgetall bike1 hmget bike1 model price weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list

weight what is returned hincrby bike1 price 100 28 list type value of kv pair is linked lists of string values use cases implementation of stacks and queues queue management message passing queues producerconsumer model logging systems easy to keep in chronological order build social media streamsfeeds message history in a chat application batch processing by queueing up a set of tasks to be executed sequentially at a later time 29 linked lists crash course back front 10 nil sequential data structure of linked nodes instead of contiguously allocated memory each node points to the next element of the list except the last one points to nilnull o1 to insert new value at front or insert new value at end 30 list commands queue queuelike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 rpop bikesrepairs rpop bilesrepairs 31 list commands stack stacklike ops lpush bikesrepairs bike1 lpush bikesrepairs bike2 lpop bikesrepairs lpop bilesrepairs 32 list commands others lpush mylist “one” lpush mylist “two” other list ops lpush mylist “three” llen mylist lrange key start stop lrange mylist 0 3 lrange mylist 0 0 lrange mylist 2 1 33 json type full support of the json standard uses jsonpath syntax for parsingnavigating a json document internally stored in binary in a treestructure → fast access to sub elements 34 set type unordered collection of unique strings members use cases track unique items ip addresses visiting a site page screen primitive relation set of all students in ds4300 access control lists for users and permission structures social network friends lists andor group membership supports set operations 35 set commands sadd ds4300 “mark” sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” sismember ds4300 “mark” sismember ds4300 “nick” scard ds4300 36 sadd ds4300 “mark” set commands sadd ds4300 “sam” sadd cs3200 “nick” sadd cs3200 “sam” scard ds4300 sinter ds4300 cs3200 sdiff ds4300 cs3200 srem ds4300 “mark” srandmember ds4300 37 38 ds 4300 redis in docker setup mark fontenot phd northeastern university prerequisites you have installed docker desktop you have installed jetbrains datagrip 2 step 1 find the redis image open docker desktop use the built in search to find the redis image click run 3 step 2 configure run the container give the new container a name enter 6379 in host port field click run give docker some time to download and start redis 4 step 3 set up data source in datagrip start datagrip create a new redis data source you can use the in the database explorer or you can use new from the file menu 5 step 4 configure the data source give the data source a name install drivers if needed message above test connection test the connection to redis there will be a message to install drivers above test connection if they aren’t already installed click ok if connection test was successful 6 ds 4300 redis python mark fontenot phd northeastern university redispy redispy is the standard client for python maintained by the redis company itself github repo redisredispy in your 4300 conda environment pip install redis 2 connecting to the server import redis redisclient redisredishost’localhost’ port6379 db2 decoderesponsestrue for your docker deployment host could be localhost or 127001 port is the port mapping given when you created the container probably the default 6379 db is the database 015 you want to connect to decoderesponses → data comes back from server as bytes setting this true converter them decodes to strings 3 redis command list full list here use filter to get to command for the particular data structure you’re targeting list hash set etc redispy documentation here the next slides are not meant to be an exhaustive list of commands only some highlights check the documentation for a complete list 4 string commands r represents the redis client object rset‘clickcountabc’ 0 val rget‘clickcountabc’ rincr‘clickcountabc’ retval rget‘clickcountabc’ printf’click count retval’ 5 string commands 2 r represents the redis client object redisclientmsetkey1 val1 key2 val2 key3 val3 printredisclientmgetkey1 key2 key3 returns as list ‘val1’ ‘val2’ ‘val3’ 6 string commands 3 set mset setex msetnx setnx get mget getex getdel incr decr incrby decrby strlen append 7 list commands 1 create list key ‘names’ values ‘mark’ ‘sam’ ‘nick’ redisclientrpushnames mark sam nick prints ‘mark’ ‘sam’ ‘nick’ printredisclientlrangenames 0 1 8 list commands 2 lpush lpop lset lrem rpush rpop lrange llen lpos other commands include moving elements between lists popping from multiple lists at the same time etc 9 hash commands 1 redisclienthsetusersession123 mappingfirst sam last uelle company redis age 30 prints name sam surname uelle company redis age 30 printredisclienthgetallusersession123 10 hash commands 2 hset hget hgetall hkeys hdel hexists hlen hstrlen 11 redis pipelines helps avoid multiple related calls to the server → less network overhead r redisredisdecoderesponsestrue pipe rpipeline for i in range5 pipesetfseati fi set5result pipeexecute printset5result true true true true true pipe rpipeline chain pipeline commands together get3result pipegetseat0getseat3getseat4execute printget3result 0 3 4 12 redis in context 13 redis in ml simplified example source httpswwwfeatureformcompostfeaturestoresexplainedthethreecommonarchitectures 14 redis in dsml source httpsmadewithmlcomcoursesmlopsfeaturestore 15 ds 4300 document databases mongodb mark fontenot phd northeastern university some material used with permission from dr rachlin with thanks document database a document database is a nonrelational database that stores data as structured documents usually in json they are designed to be simple flexible and scalable 2 what is json ● json javascript object notation ○ a lightweight datainterchange format ○ it is easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson

easy for humans to read and write ○ it is easy for machines to parse and generate ● json is built on two structures ○ a collection of namevalue pairs in various languages this is operationalized as an object record struct dictionary hash table keyed list or associative array ○ an ordered list of values in most languages this is operationalized as an array vector list or sequence ● these are two universal data structures supported by virtually all modern programming languages ○ thus json makes a great data interchange format 3 json syntax httpswwwjsonorgjsonenhtml 4 binary json bson bson → binary json binaryencoded serialization of a jsonlike document structure supports extended types not part of basic json eg date binarydata etc lightweight keep space overhead to a minimum traversable designed to be easily traversed which is vitally important to a document db efficient encoding and decoding must be efficient supported by many modern programming languages 5 xml extensible markup language ● precursor to json as data exchange format ● xml css → web pages that separated content and formatting ● structurally similar to html but tag set is extensible 6 xmlrelated toolstechnologies xpath a syntax for retrieving specific elements from an xml doc xquery a query language for interrogating xml documents the sql of xml dtd document type definition a language for describing the allowed structure of an xml document xslt extensible stylesheet language transformation tool to transform xml into other formats including nonxml formats such as html 7 why document databases document databases address the impedance mismatch problem between object persistence in oo systems and how relational dbs structure data oo programming → inheritance and composition of types how do we save a complex object to a relational database we basically have to deconstruct it the structure of a document is selfdescribing they are wellaligned with apps that use jsonxml as a transport layer 8 mongodb 9 mongodb started in 2007 after doubleclick was acquired by google and 3 of its veterans realized the limitations of relational databases for serving 400000 ads per second mongodb was short for humongous database mongodb atlas released in 2016 → documentdb as a service httpswwwmongodbcomcompanyourstory 10 mongodb structure database collection a collection b collection c document 1 document 1 document 1 document 2 document 2 document 2 document 3 document 3 document 3 11 mongodb documents no predefined schema for documents is needed every document in a collection could have different dataschema 12 relational vs mongodocument db rdbms mongodb database database tableview collection row document column field index index join embedded document foreign key reference 13 mongodb features rich query support robust support for all crud ops indexing supports primary and secondary indices on document fields replication supports replica sets with automatic failover load balancing built in 14 mongodb versions ● mongodb atlas ○ fully managed mongodb service in the cloud dbaas ● mongodb enterprise ○ subscriptionbased selfmanaged version of mongodb ● mongodb community ○ sourceavailable freetouse selfmanaged 15 interacting with mongodb ● mongosh → mongodb shell ○ cli tool for interacting with a mongodb instance ● mongodb compass ○ free opensource gui to work with a mongodb database ● datagrip and other 3rd party tools ● every major language has a library to interface with mongodb ○ pymongo python mongoose javascriptnode … 16 mongodb community edition in docker create a container map hostcontainer port 27017 e give initial username and d password for superuser 17 mongodb compass gui tool for interacting with mongodb instance download and install from here 18 load mflix sample data set in compass create a new database named mflix download mflix sample dataset and unzip it import json files for users theaters movies and comments into new collections in the mflix database 19 creating a database and collection to create a new db mflix users to create a new collection 20 mongosh mongo shell find is like select collectionfind filters projections 21 mongosh find select from users use mflix dbusersfind 22 select mongosh find from users where name “davos seaworth” filter dbusersfindname davos seaworth 23 mongosh find select from movies where rated in pg pg13 dbmoviesfindrated in pg pg13 24 mongosh find return movies which were released in mexico and have an imdb rating of at least 7 dbmoviesfind countries mexico imdbrating gte 7 25 mongosh find return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviesfind “year” 2010 or awardswins gte 5 “genres” drama 26 comparison operators 27 mongosh countdocuments how many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama 28 mongosh project return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of drama dbmoviescountdocuments “year” 2010 or awardswins gte 5 “genres” drama “name” 1 “id” 0 1 return 0 don’t return 29 pymongo 30 pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 31 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ collection db‘mycollection’ 32 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in

collectioninsertonepostinsertedid printpostid 33 count documents in collection select count from collection demodbcollectioncountdocuments 34 35 ds 4300 mongodb pymongo mark fontenot phd northeastern university pymongo ● pymongo is a python library for interfacing with mongodb instances from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ 2 getting a database and collection from pymongo import mongoclient client mongoclient ‘mongodbusernamepwlocalhost27017’ db client‘ds4300’ or clientds4300 collection db‘mycollection’ or dbmycollection 3 inserting a single document db client‘ds4300’ collection db‘mycollection’ post “author” “mark” “text” “mongodb is cool” “tags” “mongodb” “python” postid collectioninsertonepostinsertedid printpostid 4 find all movies from 2000 from bsonjsonutil import dumps find all movies released in 2000 movies2000 dbmoviesfindyear 2000 print results printdumpsmovies2000 indent 2 5 jupyter time activate your ds4300 conda or venv python environment install pymongo with pip install pymongo install jupyter lab in you python environment pip install jupyterlab download and unzip this zip file contains 2 jupyter notebooks in terminal navigate to the folder where you unzipped the files and run jupyter lab 6 7 ds 4300 introduction to the graph data model mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 what is a graph database data model based on the graph data structure composed of nodes and edges edges connect nodes each is uniquely identified each can contain properties eg name occupation etc supports queries based on graphoriented operations traversals shortest path lots of others 2 where do graphs show up social networks yes… things like instagram but also… modeling social interactions in fields like psychology and sociology the web it is just a big graph of “pages” nodes connected by hyperlinks edges chemical and biological data systems biology genetics etc interaction relationships in chemistry 3 basics of graphs and graph theory 4 what is a graph labeled property graph composed of a set of node vertex objects and relationship edge objects labels are used to mark a node as part of a group properties are attributes think kv pairs and can exist on nodes and relationships nodes with no associated relationships are ok edges not connected to nodes are not permitted 5 example 2 labels person car 4 relationship types drives owns liveswith marriedto properties 6 paths a path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated 3 1 2 ex 1 → 2 → 6 → 5 4 not a path 6 5 1 → 2 → 6 → 2 → 3 7 flavors of graphs connected vs disconnected – there is a path between any two nodes in the graph weighted vs unweighted – edge has a weight property important for some algorithms directed vs undirected – relationships edges define a start and end node acyclic vs cyclic – graph contains no cycles 8 connected vs disconnected 9 weighted vs unweighted 10 directed vs undirected 11 cyclic vs acyclic 12 sparse vs dense 13 trees 14 types of graph algorithms pathfinding pathfinding finding the shortest path between two nodes if one exists is probably the most common operation “shortest” means fewest edges or lowest weight average shortest path can be used to monitor efficiency and resiliency of networks minimum spanning tree cycle detection maxmin flow… are other types of pathfinding 15 bfs vs dfs 16 shortest path 17 types of graph algorithms centrality community detection centrality determining which nodes are “more important” in a network compared to other nodes ex social network influencers community detection evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18 centrality 19 some famous graph algorithms dijkstra’s algorithm singlesource shortest path algo for positively weighted graphs a algorithm similar to dijkstra’s with added feature of using a heuristic to guide traversal pagerank measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 21 22 ds 4300 neo4j mark fontenot phd northeastern university material referenced from graph algorithms practical examples in apache spark and neo4j by needham and hodler o’reilly press 2019 neo4j a graph database system that supports both transactional and analytical processing of graphbased data relatively new class of nosql dbs considered schema optional one can be imposed supports various types of indexing acid compliant supports distributed computing similar microsoft cosmodb amazon neptune 2 neo4j query language and plugins cypher neo4j’s graph query language created in 2011 goal sqlequivalent language for graph databases provides a visual way of matching patterns and relationships nodesconnecttoothernodes apoc plugin awesome procedures on cypher addon library that provides hundreds of procedures and functions graph data science plugin provides efficient implementations of common graph algorithms like the ones we talked about yesterday 3 neo4j in docker compose 4 docker compose ● supports multicontainer management ● setup is declarative using yaml dockercomposeyaml file ○ services ○ volumes ○ networks etc ● 1 command can be used to start stop or scale a number of services at one time ● provides a consistent method for producing an identical environment no more “well… it works on my machine ● interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose

interaction is mostly via command line 5 dockercomposeyaml services never put “secrets” in a neo4j containername neo4j docker compose file use env image neo4jlatest ports files 74747474 76877687 environment neo4jauthneo4jneo4jpassword neo4japocexportfileenabledtrue neo4japocimportfileenabledtrue neo4japocimportfileuseneo4jconfigtrue neo4jpluginsapoc graphdatascience volumes neo4jdbdatadata neo4jdblogslogs neo4jdbimportvarlibneo4jimport neo4jdbpluginsplugins 6 env files env files stores a collection of environment variables good way to keep environment variables for different platforms separate envlocal env file envdev envprod neo4jpasswordabc123 7 docker compose commands ● to test if you have docker cli properly installed run docker version ● major docker commands ○ docker compose up ○ docker compose up d ○ docker compose down ○ docker compose start ○ docker compose stop ○ docker compose build ○ docker compose build nocache 8 localhost7474 9 neo4j browser localhost7474 then login httpsneo4jcomdocsbrowsermanualcurrentvisualtour 10 inserting data by creating nodes create user name alice birthplace paris create user name bob birthplace london create user name carol birthplace london create user name dave birthplace london create user name eve birthplace rome 11 adding an edge with no variable names create user name alice birthplace paris create user name bob birthplace london match aliceuser name”alice” match bobuser name “bob” create aliceknows since “20221201”bob note relationships are directed in neo4j 12 matching which users were born in london match usruser birthplace “london” return usrname usrbirthplace 13 download dataset and move to import folder clone this repo httpsgithubcompacktpublishinggraphdatasciencewithneo4j in chapter02data of data repo unzip the netflixzip file copy netflixtitlescsv into the following folder where you put your docker compose file neo4jdbneo4jdbimport 14 importing data 15 basic data importing type the following into the cypher editor in neo4j browser load csv with headers from filenetflixtitlescsv as line createmovie id lineshowid title linetitle releaseyear linereleaseyear 16 loading csvs general syntax load csv with headers from filefileinimportfoldercsv as line fieldterminator do stuffs with line 17 importing with directors this time load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname create person name trimdirectorname but this generates duplicate person nodes a director can direct more than 1 movie 18 importing with directors merged match pperson delete p load csv with headers from filenetflixtitlescsv as line with splitlinedirector as directorslist unwind directorslist as directorname merge person name directorname 19 adding edges load csv with headers from filenetflixtitlescsv as line match mmovie id lineshowid with m splitlinedirector as directorslist unwind directorslist as directorname match pperson name directorname create pdirectedm 20 gut check let’s check the movie titled ray match mmovie title raydirectedpperson return m p 21 22 ds 4300 aws introduction mark fontenot phd northeastern university amazon web services ● leading cloud platform with over 200 different services available ● globally available via its massive networks of regions and availability zones with their massive data centers ● based on a payasyouuse cost model ○ theoretically cheaper than renting rackspaceservers in a data center… theoretically 2 history of aws ● originally launched in 2006 with only 2 services s3 ec2 ● by 2010 services had expanded to include simpledb elastic block store relational database service dynamodb cloudwatch simple workflow cloudfront availability zones and others ● amazon had competitions with big prizes to spur the adoption of aws in its early days ● they’ve continuously innovated always introducing new services for ops dev analytics etc… 200 services now 3 aws service categories 4 cloud models ● iaas more infrastructure as a service ○ contains the basic services that are needed to build an it infrastructure ● paas more platform as a service ○ remove the need for having to manage infrastructure ○ you can get right to deploying your app ● saas more software as a service ○ provide full software apps that are run and managed by another partyvendor 5 cloud models httpsbluexpnetappcomiaas 6 the shared responsibility model aws aws responsibilities security of the cloud security of physical infrastructure infra and network keep the data centers secure control access to them maintain power availability hvac etc monitor and maintain physical networking equipment and global infraconnectivity hypervisor host oss manage the virtualization layer used in aws compute services maintaining underlying host oss for other services maintaining managed services keep infra up to date and functional maintain server software patching etc 7 the shared responsibility model client client responsibilities security in the cloud control of datacontent client controls how its data is classified encrypted and shared implement and enforce appropriate datahandling policies access management iam properly configure iam users roles and policies enforce the principle of least privilege manage selfhosted apps and associated oss ensure network security to its vpc handle compliance and governance policies and procedures 8 the aws global infrastructure regions distinct geographical areas useast1 uswest 1 etc availability zones azs each region has multiple azs roughly equiv to isolated data centers edge locations locations for cdn and other types of caching services allows content to be closer to end user 9 httpsawsamazoncomaboutawsglobalinfrastructure 10 compute services vmbased ec2 ec2 spot elastic cloud compute containerbased ecs elastic container service ecr elastic container registry eks elastic kubernetes service fargate serverless container service serverless aws lambda httpsawsamazoncomproductscompute 11 storage services ● amazon s3 simple storage service ○ object storage in buckets highly scalable different storage classes ● amazon efs elastic file system ○ simple serverless elastic “setandforget” file system ● amazon ebs elastic block storage ○ highperformance block storage service ● amazon file cache ○ highspeed cache for datasets stored anywhere ● aws backup ○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting

○ fully managed policybased service to automate data protection and compliance of apps on aws httpsawsamazoncomproductsstorage 12 database services ● relational amazon rds amazon aurora ● keyvalue amazon dynamodb ● inmemory amazon memorydb amazon elasticache ● document amazon documentdb compat with mongodb ● graph amazon neptune 13 analytics services ● amazon athena analyze petabyte scale data where it lives s3 for example ● amazon emr elastic mapreduce access apache spark hive presto etc ● aws glue discover prepare and integrate all your data ● amazon redshift data warehousing service ● amazon kinesis realtime data streaming ● amazon quicksight cloudnative bireporting tool 14 ml and ai services amazon sagemaker fullymanaged ml platform including jupyter nbs build train deploy ml models aws ai services w pretrained models amazon comprehend nlp amazon rekognition imagevideo analysis amazon textract text extraction amazon translate machine translation 15 important services for data analyticsengineering ec2 and lambda amazon s3 amazon rds and dynamodb aws glue amazon athena amazon emr amazon redshift 16 aws free tier ● allows you to gain handson experience with a subset of the services for 12 months service limitations apply as well ○ amazon ec2 750 hoursmonth specific oss and instance sizes ○ amazon s3 5gb 20k gets 2k puts ○ amazon rds 750 hoursmonth of db use within certain limits ○ … so many free services 17 18 ds 4300 amazon ec2 lambda mark fontenot phd northeastern university based in part on material from gareth eagar’s data engineering with aws packt publishing ec2 2 ec2 ● ec2 → elastic cloud compute ● scalable virtual computing in the cloud ● many many instance types available ● payasyougo model for pricing ● multiple different operating systems 3 features of ec2 ● elasticity easily and programmatically scale instances up or down as needed ● you can use one of the standard amis or provide your own ami if preconfig is needed ● easily integrates with many other services such as s3 rds etc ami amazon machine image 4 ec2 lifecycle ● launch when starting an instance for the first time with a chosen configuration ● startstop temporarily suspend usage without deleting the instance ● terminate permanently delete the instance ● reboot restart an instance without sling the data on the root volume 5 where can you store data instance store temporary highspeed storage tied to the instance lifecycle efs elastic file system support shared file storage ebs elastic block storage persistent blocklevel storage s3 large data set storage or ec2 backups even 6 common ec2 use cases ● web hosting run a websiteweb server and associated apps ● data processing it’s a vm… you can do anything to data possible with a programming language ● machine learning train models using gpu instances ● disaster recovery backup critical workloads or infrastructure in the cloud 7 let’s spin up an ec2 instance 8 let’s spin up an ec2 instance 9 let’s spin up an ec2 instance 10 ubuntu vm commands initial user is ubuntu access super user commands with sudo package manager is apt kind of like homebrew or choco update the packages installed sudo apt update sudo apt upgrade 11 miniconda on ec2 make sure you’re logged in to your ec2 instance ● let’s install miniconda ○ curl o httpsrepoanacondacomminicondaminiconda3latestlinuxx8664sh ○ bash miniconda3latestlinuxx8664sh 12 installing using streamlit ● log out of your ec2 instance and log back in ● make sure pip is now available ○ pip version ● install streamlit and sklearn ○ pip install streamlit scikitlearn ● make a directory for a small web app ○ mkdir web ○ cd web 13 basic streamlit app import streamlit as st def main ● nano testpy sttitlewelcome to my streamlit app stwrite data sets ● add code on left stwrite data set 01 ● ctrlx to save and exit data set 02 data set 03 ● streamlit run testpy stwriten stwrite goodbye if name main main 14 opening up the streamlit port 15 in a browser 16 aws lambda 17 lambdas ● lambdas provide serverless computing ● automatically run code in response to events ● relieves you from having to manager servers only worry about the code ● you only pay for execution time not for idle compute time different from ec2 18 lambda features ● eventdriven execution can be triggered by many different events in aws ● supports a large number of runtimes… python java nodejs etc ● highly integrated with other aws services ● extremely scalable and can rapidly adjust to demands 19 how it works ● addupload your code through aws mgmt console ● configure event sources ● watch your lambda run when one of the event sources fires an event 20 let’s make one 21 making a lambda 22 creating a function 23 sample code ● edit the code ● deploy the code 24 test it 25 26 31325 525 pm btrees btrees the idea we saw earlier of putting multiple set list hash table elements together into large chunks that exploit locality can also be applied to trees binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line btrees are a way to get better locality by putting multiple elements into each tree node btrees were originally invented for storing data structures on disk where locality is even more crucial than with memory accessing a disk location takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which

takes about 5ms 5000000ns therefore if you are storing a tree on disk you want to make sure that a given disk read is as effective as possible btrees have a high branching factor much larger than 2 which ensures that few disk reads are needed to navigate to the place where data is stored b trees may also useful for inmemory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when btrees were first introduced a btree of order m is a search tree in which each nonleaf node has up to m children the actual elements of the collection are stored in the leaves of the tree and the nonleaf nodes contain only keys each leaf stores some number of elements the maximum number may be greater or typically less than m the data structure satisfies several invariants 1 every path from the root to a leaf has the same length 2 if a node has n children it contains n−1 keys 3 every node except the root is at least half full 4 the elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer this generalizes the bst invariant 5 the root has at least two children if it is not a leaf for example the following is an order5 btree m5 where the leaves have enough space to store up to 3 data records because the height of the tree is uniformly the same and every node is at least half full we are guaranteed that the asymptotic performance is olg n where n is the size of the collection the real win is in the constant factors of course we can choose m so that the pointers to the m children plus the m−1 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits for example if we are accessing a large disk database then our cache lines are memory blocks of the size that is read from disk lookup in a btree is straightforward given a node to start from we use a simple linear or binary search to find whether the desired element is in the node or if not which child pointer httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 12 31325 525 pm btrees to follow from the current node insertion and deletion from a btree are more complicated in fact they are notoriously difficult to implement correctly for insertion we first find the appropriate leaf node into which the inserted element falls assuming it is not already in the tree if there is already room in the node the new element can be inserted simply otherwise the current leaf is already full and must be split into two leaves one of which acquires the new element the parent is then updated to contain a new key and child pointer if the parent is already full the process ripples upwards eventually possibly reaching the root if the root is split into two then a new root is created with just two children increasing the height of the tree by one for example here is the effect of a series of insertions the first insertion 13 merely affects a leaf the second insertion 14 overflows the leaf and adds a key to an internal node the third insertion propagates all the way to the root deletion works in the opposite way the element is removed from the leaf if the leaf becomes empty a key is removed from the parent node if that breaks invariant 3 the keys of the parent node and its immediate right or left sibling are reapportioned among them so that invariant 3 is satisfied if this is not possible the parent node can be combined with that sibling removing a key another level up in the tree and possible causing a ripple all the way to the root if the root has just two children and they are combined then the root is deleted and the new combined node becomes the root of the tree reducing the height of the tree by one further reading aho hopcroft and ullman data structures and algorithms chapter 11 httpswwwcscornelleducoursescs31102012sprecitationsrec25btreesrec25html 22 31325 525 pm 126 btrees — cs3 data structures algorithms 126 btrees 1261 btrees this module presents the btree btrees are usually attributed to r bayer and e mccreight who described the btree in a 1972 paper by 1979 btrees had replaced virtually all largefile access methods other than hashing btrees or some variant of btrees are the standard file organization for applications requiring insertion deletion and key range searches they are used to implement most modern file systems btrees address effectively all of the major problems encountered when implementing diskbased search trees 1 the btree is shallow in part because the tree is always height balanced all leaf nodes are at the same level and in part because the branching factor is quite high so only a small number of disk blocks are accessed to reach a given record 2 update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record the fewer the number of disk blocks affected during an operation the less disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root

disk io is required 3 btrees keep related records that is records with similar key values on the same disk block which helps to minimize disk io on range searches 4 btrees guarantee that every node in the tree will be full at least to a certain minimum percentage this improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation a btree of order m is defined to have the following shape properties the root is either a leaf or has at least two children each internal node except for the root has between ⌈m2⌉ and m children all leaves are at the same level in the tree so the tree is always height balanced the btree is a generalization of the 23 tree put another way a 23 tree is a btree of order three normally the size of a node in the b tree is chosen to fill a disk block a btree node implementation typically allows 100 or more children thus a btree node is equivalent to a disk block and a “pointer” value stored in the tree is actually the number of the block containing the child node usually interpreted as an offset from the beginning of the corresponding disk file in a typical application the btree’s access to the disk file will be managed using a buffer pool and a blockreplacement scheme such as lru figure 1261 shows a btree of order four each node contains up to three keys and internal nodes have up to four children 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 figure 1261 a btree of order four search in a btree is a generalization of search in a 23 tree it is an alternating twostep process beginning with the root node of the b tree 1 perform a binary search on the records in the current node if a record with the search key is found then return that record if the current node is a leaf node and the key is not found then report an unsuccessful search httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 19 31325 525 pm 126 btrees — cs3 data structures algorithms 2 otherwise follow the proper branch and repeat the process for example consider a search for the record with key value 47 in the tree of figure 1261 the root node is examined and the second right branch taken after examining the node at level 1 the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47 btree insertion is a generalization of 23 tree insertion the first step is to find the leaf node that should contain the key to be inserted space permitting if there is room in this node then insert the key if there is not then split the node into two and promote the middle key to the parent if the parent becomes full then it is split in turn and its middle key promoted note that this insertion process is guaranteed to keep all nodes at least half full for example when we attempt to insert into a full internal node of a btree of order four there will now be five children that must be dealt with the node is split into two nodes containing two keys each thus retaining the btree property the middle of the five children is promoted to its parent 12611 b trees the previous section mentioned that btrees are universally used to implement largescale diskbased systems actually the btree as described in the previous section is almost never implemented what is most commonly implemented is a variant of the btree called the b tree when greater efficiency is required a more complicated variant known as the b∗ tree is used consider again the linear index when the collection of records will not change a linear index provides an extremely efficient way to search the problem is how to handle those pesky inserts and deletes we could try to keep the core idea of storing a sorted array based list but make it more flexible by breaking the list into manageable chunks that are more easily updated how might we do that first we need to decide how big the chunks should be since the data are on disk it seems reasonable to store a chunk that is the size of a disk block or a small multiple of the disk block size if the next record to be inserted belongs to a chunk that hasn’t filled its block then we can just insert it there the fact that this might cause other records in that chunk to move a little bit in the array is not important since this does not cause any extra disk accesses so long as we move data within that chunk but what if the chunk fills up the entire block that contains it we could just split it in half what if we want to delete a record we could just take the deleted record out of the chunk but we might not want a lot of nearempty chunks so we could put adjacent chunks together if they have only a small amount of data between them or we could shuffle data between adjacent chunks that together contain more data the big problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to

problem would be how to find the desired chunk when processing a record with a given key perhaps some sort of treelike structure could be used to locate the appropriate chunk these ideas are exactly what motivate the b tree the b tree is essentially a mechanism for managing a sorted arraybased list where the list is broken into chunks the most significant difference between the b tree and the bst or the standard btree is that the b tree stores records only at the leaf nodes internal nodes store key values but these are used solely as placeholders to guide the search this means that internal nodes are significantly different in structure from leaf nodes internal nodes store keys to guide the search associating each key with a pointer to a child b tree node leaf nodes store actual records or else keys and pointers to actual records in a separate disk file if the b tree is being used purely as an index depending on the size of a record as compared to the size of a key a leaf node in a b tree of order m might have enough room to store more or less than m records the requirement is simply that the leaf nodes store enough records to remain at least half full the leaf nodes of a b tree are normally linked together to form a doubly linked list thus the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list here is a javalike pseudocode representation for the b tree node interface leaf node and internal node subclasses would implement this interface interface for b tree nodes public interface bpnodekeye public boolean isleaf public int numrecs public key keys an important implementation detail to note is that while figure 1261 shows internal nodes containing three keys and four pointers class bpnode is slightly different in that it stores keypointer pairs figure 1261 shows the b tree as it is traditionally drawn to simplify implementation in practice nodes really do associate a key with each pointer each internal node should be assumed to hold in the httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 29 31325 525 pm 126 btrees — cs3 data structures algorithms leftmost position an additional key that is less than or equal to any possible key value in the node’s leftmost subtree b tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value let’s see in some detail how the simplest b tree works this would be the “2−3 tree” or a b tree of order 3 1 28 example 23 tree visualization insert figure 1262 an example of building a 2−3 tree next let’s see how to search 1 10 example 23 tree visualization search 46 65 33 52 71 15 22 33 46 47 52 65 71 89 j x o h l b s w m figure 1263 an example of searching a 2−3 tree finally let’s see an example of deleting from the 2−3 tree 1 33 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 39 31325 525 pm 126 btrees — cs3 data structures algorithms example 23 tree visualization delete 46 65 22 51 71 figure 1264 an example of deleting from a 2−3 tree now let’s extend these ideas to a b tree of higher order b trees are exceptionally good for range queries once the first record in the range has been found the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node and then continuing down the linked list of leaf nodes as far as necessary figure illustrates the b tree 1 10 example b tree visualization search in a tree of degree 4 77 25 40 98 10 18 25 39 40 55 77 89 98 127 s e t f q f a b a v figure 1265 an example of search in a b tree of order four internal nodes must store between two and four children search in a b tree is nearly identical to search in a regular btree except that the search must always continue to the proper leaf node even if the searchkey value is found in an internal node this is only a placeholder and does not provide access to the actual record here is a pseudocode sketch of the b tree search algorithm private e findhelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 49 31325 525 pm 126 btrees — cs3 data structures algorithms return bpleafkeyertrecscurrec else return null else return findhelpbpinternalkeyertpointerscurrec k b tree insertion is similar to btree insertion first the leaf l that should contain the record is found if l is not full then the new record is added and no other b tree nodes are affected if l is already full split it in two dividing the records evenly among the two nodes and promote a copy of the leastvalued key in the newly formed right node as with the 23 tree promotion might cause the parent to split in turn perhaps eventually leading to splitting the root and causing the b tree to gain a new level b tree insertion keeps all leaf nodes at equal depth figure illustrates the insertion process through several examples 1 42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if

42 example b tree visualization insert into a tree of degree 4 figure 1266 an example of building a b tree of order four here is a a javalike pseudocode sketch of the b tree insert algorithm private bpnodekeye inserthelpbpnodekeye rt key k e e bpnodekeye retval if rtisleaf at leaf node insert here return bpleafkeyertaddk e add to internal node int currec binarylertkeys rtnumrecs k bpnodekeye temp inserthelp bpinternalkeyerootpointerscurrec k e if temp bpinternalkeyertpointerscurrec return bpinternalkeyert addbpinternalkeyetemp else return rt httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 59 31325 525 pm 126 btrees — cs3 data structures algorithms here is an exercise to see if you get the basic idea of b tree insertion b tree insertion instructions in this exercise your job is to insert the values from the stack to the b tree search for the leaf node where the topmost value of the stack should be inserted and click on that node the exercise will take care of the rest continue this procedure until you have inserted all the values in the stack undo reset model answer grade 91743554471068713459 16 60 48 82 65 38 69 77 to delete record r from the b tree first locate the leaf l that contains r if l is more than half full then we need only remove r leaving l still at least half full this is demonstrated by figure 1 23 example b tree visualization delete from a tree of degree 4 58 12 44 67 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 69 31325 525 pm 126 btrees — cs3 data structures algorithms 5 10 12 27 44 48 58 60 67 88 figure 1267 an example of deletion in a b tree of order four if deleting a record reduces the number of records in the node below the minimum threshold called an underflow then we must do something to keep the node sufficiently full the first choice is to look at the node’s adjacent siblings to determine if they have a spare record that can be used to fill the gap if so then enough records are transferred from the sibling so that both nodes have about the same number of records this is done so as to delay as long as possible the next time when a delete causes this node to underflow again this process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node if neither sibling can lend a record to the underfull node call it n then n must give its records to a sibling and be removed from the tree there is certainly room to do this because the sibling is at most half full remember that it had no records to contribute to the current node and n has become less than half full because it is underflowing this merge process combines two subtrees of the parent which might cause it to underflow in turn if the last two children of the root merge together then the tree loses a level here is a javalike pseudocode for the b tree delete algorithm delete a record with the given key value and return true if the root underflows private boolean removehelpbpnodekeye rt key k int currec binarylertkeys rtnumrecs k if rtisleaf if bpleafkeyertkeyscurrec k return bpleafkeyertdeletecurrec else return false else process internal node if removehelpbpinternalkeyertpointerscurrec k child will merge if necessary return bpinternalkeyertunderflowcurrec else return false the b tree requires that all nodes be at least half full except for the root thus the storage utilization must be at least 50 this is satisfactory for many implementations but note that keeping nodes fuller will result both in less space required because there is less empty space in the disk file and in more efficient processing fewer blocks on average will be read into memory because the amount of information in each block is greater because btrees have become so popular many algorithm designers have tried to improve btree performance one method for doing so is to use the b tree variant known as the b∗ tree the b∗ tree is identical to the b tree except for the rules used to split and merge nodes instead of splitting a node in half when it overflows the b∗ tree gives some records to its neighboring sibling if possible if the sibling is also full then these two nodes split into three similarly when a node underflows it is combined with its two siblings and the total reduced to two nodes thus the nodes are always at least two thirds full 1 finally here is an example of building a b tree of order five you can compare this to the example above of building a tree of order four with the same records 1 33 example b tree visualization insert into a tree of degree 5 httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 79 31325 525 pm 126 btrees — cs3 data structures algorithms figure 1268 an example of building a b tree of degree 5 click here for a visualization that will let you construct and interact with a b tree this visualization was written by david galles of the university of san francisco as part of his data structure visualizations package 1 this concept can be extended further if higher space utilization is required however the update routines become much more complicated i once worked on a project where we implemented 3for4 node split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or

split and merge routines this gave better performance than the 2for3 node split and merge routines of the b∗ tree however the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed 12612 btree analysis the asymptotic cost of search insertion and deletion of records from btrees b trees and b∗ trees is θlogn where n is the total number of records in the tree however the base of the log is the average branching factor of the tree typical database applications use extremely high branching factors perhaps 100 or more thus in practice the btree and its variants are extremely shallow as an illustration consider a b tree of order 100 and leaf nodes that contain up to 100 records a bb tree with height one that is just a single leaf node can have at most 100 records a b tree with height two a root internal node whose children are leaves must have at least 100 records 2 leaves with 50 records each it has at most 10000 records 100 leaves with 100 records each a b tree with height three must have at least 5000 records two secondlevel nodes with 50 children containing 50 records each and at most one million records 100 secondlevel nodes with 100 full children each a b tree with height four must have at least 250000 records and at most 100 million records thus it would require an extremely large database to generate a b tree of more than height four the b tree split and insert rules guarantee that every node except perhaps the root is at least half full so they are on average about 34 full but the internal nodes are purely overhead since the keys stored there are used only by the tree to direct search rather than store actual data does this overhead amount to a significant use of space no because once again the high fanout rate of the tree structure means that the vast majority of nodes are leaf nodes a kary tree has approximately 1k of its nodes as internal nodes this means that while half of a full binary tree’s nodes are internal nodes in a b tree of order 100 probably only about 175 of its nodes are internal nodes this means that the overhead associated with internal nodes is very low we can reduce the number of disk fetches required for the btree even more by using the following methods first the upper levels of the tree can be stored in main memory at all times because the tree branches so quickly the top two levels levels 0 and 1 require relatively little space if the btree is only height four then at most two disk fetches internal nodes at level two and leaves at level three are required to reach the pointer to any given record a buffer pool could be used to manage nodes of the btree several nodes of the tree would typically be in main memory at one time the most straightforward approach is to use a standard method such as lru to do node replacement however sometimes it might be desirable to “lock” certain nodes such as the root into the buffer pool in general if the buffer pool is even of modest size say at least twice the depth of the tree no special techniques for node replacement will be required because the upperlevel nodes will naturally be accessed frequently httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 89 31325 525 pm 126 btrees — cs3 data structures algorithms httpsopendsaservercsvteduodsabookscs3htmlbtreehtmlid2 99 chapter 12 binary search trees a binary search tree is a binary tree with a special property called the bstproperty which is given as follows ⋆ for all nodes x and y if y belongs to the left subtree of x then the key at y is less than the key at x and if y belongs to the right subtree of x then the key at y is greater than the key at x we will assume that the keys of a bst are pairwise distinct each node has the following attributes p left and right which are pointers to the parent the left child and the right child respectively and key which is key stored at the node 1 an example 7 4 12 2 6 9 19 3 5 8 11 15 20 2 traversal of the nodes in a bst by “traversal” we mean visiting all the nodes in a graph traversal strategies can be specified by the ordering of the three objects to visit the current node the left subtree and the right subtree we assume the the left subtree always comes before the right subtree then there are three strategies 1 inorder the ordering is the left subtree the current node the right subtree 2 preorder the ordering is the current node the left subtree the right subtree 3 postorder the ordering is the left subtree the right subtree the current node 3 inorder traversal pseudocode this recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree while doing traversal it prints out the key of each node that is visited inorderwalkx 1 if x nil then return 2 inorderwalkleftx 3 print keyx 4 inorderwalkrightx we can write a similar pseudocode for preorder and postorder 4 2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys

2 1 3 1 3 2 3 1 2 inorder preorder postorder 7 4 12 2 6 9 19 3 5 8 11 15 20 what is the outcome of inorder traversal on this bst how about postorder traversal and preorder traversal 5 inorder traversal gives 2 3 4 5 6 7 8 9 11 12 15 19 20 preorder traversal gives 7 4 2 3 6 5 12 9 8 11 19 15 20 postorder traversal gives 3 2 5 6 4 8 11 9 15 20 19 12 7 so inorder travel on a bst finds the keys in nondecreasing order 6 operations on bst 1 searching for a key we assume that a key and the subtree in which the key is searched for are given as an input we’ll take the full advantage of the bstproperty suppose we are at a node if the node has the key that is being searched for then the search is over otherwise the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for if the former is the case then by the bst property all the keys in th left subtree are strictly less than the key that is searched for that means that we do not need to search in the left subtree thus we will examine only the right subtree if the latter is the case by symmetry we will examine only the right subtree 7 algorithm here k is the key that is searched for and x is the start node bstsearchx k 1 y x ← 2 while y nil do ̸ 3 if keyy k then return y 4 else if keyy k then y righty ← 5 else y lefty ← 6 return “not found” 8 an example 7 search for 8 4 11 2 6 9 13 nil what is the running time of search 9 2 the maximum and the minimum to find the minimum identify the leftmost node ie the farthest node you can reach by following only left branches to find the maximum identify the rightmost node ie the farthest node you can reach by following only right branches bstminimumx 1 if x nil then return “empty tree” 2 y x ← 3 while lefty nil do y lefty ̸ ← 4 return keyy bstmaximumx 1 if x nil then return “empty tree” 2 y x ← 3 while righty nil do y righty ̸ ← 4 return keyy 10 3 insertion suppose that we need to insert a node z such that k keyz using binary search we find a nil such that replacing it by z does not break the bstproperty 11 bstinsertx z k 1 if x nil then return “error” 2 y x ← 3 while true do 4 if keyy k 5 then z lefty ← 6 else z righty ← 7 if z nil break 8 9 if keyy k then lefty z ← 10 else rightpy z ← 12 4 the successor and the predecessor the successor respectively the predecessor of a key k in a search tree is the smallest respectively the largest key that belongs to the tree and that is strictly greater than respectively less than k the idea for finding the successor of a given node x if x has the right child then the successor is the minimum in the right subtree of x otherwise the successor is the parent of the farthest node that can be reached from x by following only right branches backward 13 an example 23 25 7 4 12 2 6 9 19 3 5 8 11 15 20 14 algorithm bstsuccessorx 1 if rightx nil then ̸ 2 y rightx ← 3 while lefty nil do y lefty ̸ ← 4 return y 5 else 6 y x ← 7 while rightpx x do y px ← 8 if px nil then return px ̸ 9 else return “no successor” 15 the predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged for which node is the successor undefined what is the running time of the successor algorithm 16 5 deletion suppose we want to delete a node z 1 if z has no children then we will just replace z by nil 2 if z has only one child then we will promote the unique child to z’s place 3 if z has two children then we will identify z’s successor call it y the successor y either is a leaf or has only the right child promote y to z’s place treat the loss of y using one of the above two solutions 17 8 8 5 11 5 11 1 6 9 13 1 6 9 13 3 7 10 3 10 2 4 2 4 8 8 5 11 5 11 1 6 9 13 3 6 9 13 3 7 10 2 4 7 10 2 4 8 9 5 11 5 11 1 6 9 13 1 6 10 13 3 7 10 3 2 4 2 4 18 algorithm this algorithm deletes z from bst t bstdeletet z 1 if leftz nil or rightz nil 2 then y z ← 3 else y bstsuccessorz ← 4 ✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16

✄ y is the node that’s actually removed 5 ✄ here y does not have two children 6 if lefty nil ̸ 7 then x lefty ← 8 else x righty ← 9 ✄ x is the node that’s moving to y’s position 10 if x nil then px py ̸ ← 11 ✄ px is reset if x isn’t nil 12 ✄ resetting is unnecessary if x is nil 19 algorithm cont’d 13 if py nil then roott x ← 14 ✄ if y is the root then x becomes the root 15 ✄ otherwise do the following 16 else if y leftpy 17 then leftpy x ← 18 ✄ if y is the left child of its parent then 19 ✄ set the parent’s left child to x 20 else rightpy x ← 21 ✄ if y is the right child of its parent then 22 ✄ set the parent’s right child to x 23 if y z then ̸ 24 keyz keyy ← 25 move other data from y to z 27 return y 20 summary of efficiency analysis theorem a on a binary search tree of height h search minimum maximum successor predecessor insert and delete can be made to run in oh time 21 randomly built bst suppose that we insert n distinct keys into an initially empty tree assuming that the n permutations are equally likely to occur what is the average height of the tree to study this question we consider the process of constructing a tree t by inserting in order randomly selected n distinct keys to an initially empty tree here the actually values of the keys do not matter what matters is the position of the inserted key in the n keys 22 the process of construction so we will view the process as follows a key x from the keys is selected uniformly at random and is inserted to the tree then all the other keys are inserted here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree thus the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree 23 random variables n number of keys x height of the tree of n keys n x y 2 n n we want an upper bound on ey n for n 2 we have ≥ n 1 ey 2emax y y n i 1 n i n ⎛ ⎞ − − i1 ⎝ ⎠ emax y y ey y i 1 n i i 1 n i ≤ − − − − ey ey i 1 n i ≤ − − collecting terms n 1 4 − ey ey n i ≤ n i1 24 analysis 1 n3 we claim that for all n 1 ey n 4 3 ≥ ≤ we prove this by induction on n ’ 0 base case ey 2 1 1 induction step we have n 1 4 − ey ey n i ≤ n i1 using the fact that n 1 i 3 n 3 − 3 4 i0 ’ ’ 4 1 n 3 ey n ≤ n · 4 · 4 ’ 1 n 3 ey n ≤ 4 · 3 ’ 25 jensen’s inequality a function f is convex if for all x and y x y and for all λ 0 λ 1 ≤ ≤ fλx 1 λy λfx 1 λfy − ≤ − jensen’s inequality states that for all random variables x and for all convex function f fex efx ≤ x let this x be x and fx 2 then n efx ey so we have n 1 n 3 ex 2 n ≤ 4 3 ’ 3 the righthand side is at most n 3 by taking the log of both sides we have ex olog n n thus the average height of a randomly build bst is olog n 26 31325 524 pm ics 46 spring 2022 notes and examples avl trees ics 46 spring 2022 news course reference schedule project guide notes and examples reinforcement exercises grade calculator about alex ics 46 spring 2022 notes and examples avl trees why we must care about binary search tree balancing weve seen previously that the performance characteristics of binary search trees can vary rather wildly and that theyre mainly dependent on the shape of the tree with the height of the tree being the key determining factor by definition binary search trees restrict what keys are allowed to present in which nodes — smaller keys have to be in left subtrees and larger keys in right subtrees — but they specify no restriction on the trees shape meaning that both of these are perfectly legal binary search trees containing the keys 1 2 3 4 5 6 and 7 yet while both of these are legal one is better than the other because the height of the first tree called a perfect binary tree is smaller than the height of the second called a degenerate tree these two shapes represent the two extremes — the best and worst possible shapes for a binary search tree containing seven keys of course when all you have is a very small number of keys like this any shape will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of

will do but as the number of keys grows the distinction between these two tree shapes becomes increasingly vital whats more the degenerate shape isnt even necessarily a rare edge case its what you get when you start with an empty tree and add keys that are already in order which is a surprisingly common scenario in realworld programs for example one very obvious algorithm for generating unique integer keys — when all you care about is that theyre unique — is to generate them sequentially whats so bad about a degenerate tree anyway just looking at a picture of a degenerate tree your intuition should already be telling you that something is amiss in particular if you tilt your head 45 degrees to the right they look just like linked lists that perception is no accident as they behave like them too except that theyre more complicated to boot from a more analytical perspective there are three results that should give us pause every time you perform a lookup in a degenerate binary search tree it will take on time because its possible that youll have to reach every node in the tree before youre done as n grows this is a heavy burden to bear if you implement your lookup recursively you might also be using on memory too as you might end up with as many as n frames on your runtime stack — one for every recursive call there are ways to mitigate this — for example some kinds of carefullywritten recursion in some programming languages including c can avoid runtime stack growth as you recurse — but its still a sign of potential trouble the time it will take you to build the degenerate tree will also be prohibitive if you start with an empty binary search tree and add keys to it in order how long does it take to do it the first key you add will go directly to the root you could think of this as taking a single step creating the node the second key you add will require you to look at the root node then take one step to the right you could think of this as taking two steps each subsequent key you add will require one more step than the one before it the total number of steps it would take to add n keys would be determined by the sum 1 2 3 n this sum which well see several times throughout this course is equal to nn 1 2 so the total number of steps to build the entire tree would be θn2 overall when n gets large the tree would be hideously expensive to build and then every subsequent search would be painful as well so this in general is a situation we need to be sure to avoid or else we should probably consider a data structure other than a binary search tree the worst case is simply too much of a burden to bear if n might get large but if we can find a way to control the trees shape more carefully to force it to remain more balanced well be fine the question of course is how to do it and as importantly whether we can do it while keeping the cost low enough that it doesnt outweigh the benefit aiming for perfection the best goal for us to shoot for would be to maintain perfection in other words every time we insert a key into our binary search tree it would ideally still be a perfect binary tree in which case wed know that the height of the tree would always be θlog n with a commensurate effect on performance httpsicsucieduthorntonics46notesavltrees 17 31325 524 pm ics 46 spring 2022 notes and examples avl trees however when we consider this goal a problem emerges almost immediately the following are all perfect binary trees by definition the perfect binary trees pictured above have 1 3 7 and 15 nodes respectively and are the only possible perfect shapes for binary trees with that number of nodes the problem though lies in the fact that there is no valid perfect binary tree with 2 nodes or with 4 5 6 8 9 10 11 12 13 or 14 nodes so generally its impossible for us to guarantee that a binary search tree will always be perfect by our definition because theres simply no way to represent most numbers of keys so first things first well need to relax our definition of perfection to accommodate every possible number of keys we might want to store complete binary trees a somewhat more relaxed notion of perfection is something called a complete binary tree which is defined as follows a complete binary tree of height h is a binary tree where if h 0 its left and right subtrees are empty if h 0 one of two things is true the left subtree is a perfect binary tree of height h − 1 and the right subtree is a complete binary tree of height h − 1 the left subtree is a complete binary tree of height h − 1 and the right subtree is a perfect binary tree of height h − 2 that can be a bit of a mindbending definition but it actually leads to a conceptually simple result on every level of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see

of a complete binary tree every node that could possibly be present will be except the last level might be missing nodes but if it is missing nodes the nodes that are there will be as far to the left as possible the following are all complete binary trees furthermore these are the only possible complete binary trees with these numbers of nodes in them any other arrangement of say 6 keys besides the one shown above would violate the definition weve seen that the height of a perfect binary tree is θlog n its not a stretch to see that the height a complete binary tree will be θlog n as well and well accept that via our intuition for now and proceed all in all a complete binary tree would be a great goal for us to attain if we could keep the shape of our binary search trees complete we would always have binary search trees with height θlog n the cost of maintaining completeness the trouble of course is that we need an algorithm for maintaining completeness and before we go to the trouble of trying to figure one out we should consider whether its even worth our time what can we deduce about the cost of maintaining completeness even if we havent figured out an algorithm yet one example demonstrates a very big problem suppose we had the binary search tree on the left — which is complete by our definition — and we wanted to insert the key 1 into it if so we would need an algorithm that would transform the tree on the left into the tree on the right the tree on the right is certainly complete so this would be the outcome wed want but consider what it would take to do it every key in the tree had to move so no matter what algorithm we used we would still have to move every key if there are n keys in the tree that would take ωn time — moving n keys takes at least linear time even if you have the best possible algorithm for moving them the work still has to get done so in the worst case maintaining completeness after a single insertion requires ωn time unfortunately this is more time than we ought to be spending on maintaining balance this means well need to come up with a compromise as is often the case when we learn or design algorithms our willingness to tolerate an imperfect result thats still good enough for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result so what would a good enough result be what is a good balance condition our overall goal is for lookups insertions and removals from a binary search tree to require olog n time in every case rather than letting them degrade to a worstcase behavior of on to do that we need to decide on a balance condition which is to say that we need to understand what shape is considered well httpsicsucieduthorntonics46notesavltrees 27 31325 524 pm ics 46 spring 2022 notes and examples avl trees enough balanced for our purposes even if not perfect a good balance condition has two properties the height of a binary search tree meeting the condition is θlog n it takes olog n time to rebalance the tree on insertions and removals in other words it guarantees that the height of the tree is still logarithmic which will give us logarithmictime lookups and the time spent rebalancing wont exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height the cost wont outweigh the benefit coming up with a balance condition like this on our own is a tall task but we can stand on the shoulders of the giants who came before us with the definition above helping to guide us toward an understanding of whether weve found what were looking for a compromise avl trees there are a few wellknown approaches for maintaining binary search trees in a state of nearbalance that meets our notion of a good balance condition one of them is called an avl tree which well explore here others which are outside the scope of this course include redblack trees which meet our definition of good and splay trees which dont always meet our definition of good but do meet it on an amortized basis but well stick with the one solution to the problem for now avl trees avl trees are what you might called nearly balanced binary search trees while they certainly arent as perfectlybalanced as possible they nonetheless achieve the goals weve decided on maintaining logarithmic height at no more than logarithmic cost so what makes a binary search tree nearly balanced enough to be considered an avl tree the core concept is embodied by something called the avl property we say that a node in a binary search tree has the avl property if the heights of its left and right subtrees differ by no more than 1 in other words we tolerate a certain amount of imbalance — heights of subtrees can be slightly different but no more than that — in hopes that we can more efficiently maintain it since were going to be comparing heights of subtrees theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an

theres one piece of background we need to consider recall that the height of a tree is the length of its longest path by definition the height of a tree with just a root node and empty subtrees would then be zero but what about a tree thats totally empty to maintain a clear pattern relative to other tree heights well say that the height of an empty tree is 1 this means that a node with say a childless left child and no right child would still be considered balanced this leads us finally to the definition of an avl tree an avl tree is a binary search tree in which all nodes have the avl property below are a few binary trees two of which are avl and two of which are not the thing to keep in mind about avl is that its not a matter of squinting at a tree and deciding whether it looks balanced theres a precise definition and the two trees above that dont meet that definition fail to meet it because they each have at least one node marked in the diagrams by a dashed square that doesnt have the avl property avl trees by definition are required to meet the balance condition after every operation every time you insert or remove a key every node in the tree should have the avl property to meet that requirement we need to restructure the tree periodically essentially detecting and correcting imbalance whenever and wherever it happens to do that we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree smaller keys toward the left larger ones toward the right rotations rebalancing of avl trees is achieved using what are called rotations which when used at the proper times efficiently improve the shape of the tree by altering a handful of pointers there are a few kinds of rotations we should first understand how they work then focus our attention on when to use them the first kind of rotation is called an ll rotation which takes the tree on the left and turns it into the tree on the right the circle with a and b written in them are each a single node containing a single key the triangles with t t and t written in them are arbitrary subtrees which may be empty or may contain any 1 2 3 number of nodes but which are themselves binary search trees httpsicsucieduthorntonics46notesavltrees 37 31325 524 pm ics 46 spring 2022 notes and examples avl trees its important to remember that both of these trees — before and after — are binary search trees the rotation doesnt harm the ordering of the keys in nodes because the subtrees t t and t maintain the appropriate positions relative to the keys a and b 1 2 3 all keys in t are smaller than a 1 all keys in t are larger than a and smaller than b 2 all keys in t are larger than b 3 performing this rotation would be a simple matter of adjusting a few pointers — notably a constant number of pointers no matter how many nodes are in the tree which means that this rotation would run in θ1 time bs parent would now point to a where it used to point to b as right child would now be b instead of the root of t 2 bs left child would now be the root of t instead of a 2 a second kind of rotation is an rr rotation which makes a similar adjustment note that an rr rotation is the mirror image of an ll rotation a third kind of rotation is an lr rotation which makes an adjustment thats slightly more complicated an lr rotation requires five pointer updates instead of three but this is still a constant number of changes and runs in θ1 time finally there is an rl rotation which is the mirror image of an lr rotation once we understand the mechanics of how rotations work were one step closer to understanding avl trees but these rotations arent arbitrary theyre used specifically to correct imbalances that are detected after insertions or removals an insertion algorithm httpsicsucieduthorntonics46notesavltrees 47 31325 524 pm ics 46 spring 2022 notes and examples avl trees inserting a key into an avl tree starts out the same way as insertion into a binary search tree perform a lookup if you find the key already in the tree youre done because keys in a binary search tree must be unique when the lookup terminates without the key being found add a new node in the appropriate leaf position where the lookup ended the problem is that adding the new node introduced the possibility of an imbalance for example suppose we started with this avl tree and then we inserted the key 35 into it a binary search tree insertion would give us this as a result but this resulting tree is not an avl tree because the node containing the key 40 does not have the avl property because the difference in the heights of its subtrees is 2 its left subtree has height 1 its right subtree — which is empty — has height 1 what can we do about it the answer lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to

lies in the following algorithm which we perform after the normal insertion process work your way back up the tree from the position where you just added a node this could be quite simple if the insertion was done recursively compare the heights of the left and right subtrees of each node when they differ by more than 1 choose a rotation that will fix the imbalance note that comparing the heights of the left and right subtrees would be quite expensive if you didnt already know what they were the solution to this problem is for each node to store its height ie the height of the subtree rooted there this can be cheaply updated after every insertion or removal as you unwind the recursion the rotation is chosen considering the two links along the path below the node where the imbalance is heading back down toward where you inserted a node if you were wondering where the names ll rr lr and rl come from this is the answer to that mystery if the two links are both to the left perform an ll rotation rooted where the imbalance is if the two links are both to the right perform an rr rotation rooted where the imbalance is if the first link is to the left and the second is to the right perform an lr rotation rooted where the imbalance is if the first link is to the right and the second is to the left perform an rl rotation rooted where the imbalance is it can be shown that any one of these rotations — ll rr lr or rl — will correct any imbalance brought on by inserting a key in this case wed perform an lr rotation — the first two links leading from 40 down toward 35 are a left and a right — rooted at 40 which would correct the imbalance and the tree would be rearranged to look like this compare this to the diagram describing an lr rotation the node containing 40 is c the node containing 30 is a the node containing 35 is b the empty left subtree of the node containing 30 is t 1 the empty left subtree of the node containing 35 is t 2 the empty right subtree of the node containing 35 is t 3 the empty right subtree of the node containing 40 is t 4 httpsicsucieduthorntonics46notesavltrees 57 31325 524 pm ics 46 spring 2022 notes and examples avl trees after the rotation we see what wed expect the node b which in our example contained 35 is now the root of the newlyrotated subtree the node a which in our example contained 30 is now the left child of the root of the newlyrotated subtree the node c which in our example contained 40 is now the right child of the root of the newlyrotated subtree the four subtrees t t t and t were all empty so they are still empty 1 2 3 4 note too that the tree is more balanced after the rotation than it was before this is no accident a single rotation ll rr lr or rl is all thats necessary to correct an imbalance introduced by the insertion algorithm a removal algorithm removals are somewhat similar to insertions in the sense that you would start with the usual binary search tree removal algorithm then find and correct imbalances while the recursion unwinds the key difference is that removals can require more than one rotation to correct imbalances but will still only require rotations on the path back up to the root from where the removal occurred — so generally olog n rotations asymptotic analysis the key question here is what is the height of an avl tree with n nodes if the answer is θlog n then we can be certain that lookups insertions and removals will take olog n time how can we be so sure lookups would be olog n because theyre the same as they are in a binary search tree that doesnt have the avl property if the height of the tree is θlog n lookups will run in olog n time insertions and removals despite being slightly more complicated in an avl tree do their work by traversing a single path in the tree — potentially all the way down to a leaf position then all the way back up if the length of the longest path — thats what the height of a tree is — is θlog n then we know that none of these paths is longer than that so insertions and removals will take olog n time so were left with that key question what is the height of an avl tree with n nodes if youre not curious you can feel free to just assume this if you want to know more keep reading what is the height of an avl tree with n nodes optional the answer revolves around noting how many nodes at minimum could be in a binary search tree of height n and still have it be an avl tree it turns out avl trees of height n ≥ 2 that have the minimum number of nodes in them all share a similar property the avl tree with height h ≥ 2 with the minimum number of nodes consists of a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with

a root node with two subtrees one of which is an avl tree with height h − 1 with the minimum number of nodes the other of which is an avl tree with height h − 2 with the minimum number of nodes given that observation we can write a recurrence that describes the number of nodes at minimum in an avl tree of height h m0 1 when height is 0 minimum number of nodes is 1 a root node with no children m1 2 when height is 1 minimum number of nodes is 2 a root node with one child and not the other mh 1 mh 1 mh 2 while the repeated substitution technique we learned previously isnt a good way to try to solve this particular recurrence we can prove something interesting quite easily we know for sure that avl trees with larger heights have a bigger minimum number of nodes than avl trees with smaller heights — thats fairly selfexplanatory — which means that we can be sure that 1 mh − 1 ≥ mh − 2 given that we can conclude the following mh ≥ 2mh 2 we can then use the repeated substitution technique to determine a lower bound for this recurrence mh ≥ 2mh 2 ≥ 22mh 4 ≥ 4mh 4 ≥ 42mh 6 ≥ 8mh 6 ≥ 2jmh 2j we could prove this by induction on j but well accept it on faith let j h2 ≥ 2h2mh h ≥ 2h2m0 mh ≥ 2h2 so weve shown that the minimum number of nodes that can be present in an avl tree of height h is at least 2h2 in reality its actually more than that but this gives us something useful to work with we can use this result to figure out what were really interested in which is the opposite what is the height of an avl tree with n nodes mh ≥ 2h2 log mh ≥ h2 2 2 log mh ≥ h 2 finally we see that for avl trees of height h with the minimum number of nodes the height is no more than 2 log n where n is the number of nodes in the 2 tree for avl trees with more than the minimum number of nodes the relationship between the number of nodes and the height is even better though for httpsicsucieduthorntonics46notesavltrees 67 31325 524 pm ics 46 spring 2022 notes and examples avl trees reasons weve seen previously we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic so ultimately we see that the height of an avl tree with n nodes is θlog n in reality it turns out that the bound is lower than 2 log n its something more akin to about 144 log n even for avl trees with the minimum number of nodes 2 2 though the proof of that is more involved and doesnt change the asymptotic result httpsicsucieduthorntonics46notesavltrees 77